# Event 1: Gen AI and Data

⚠️ **Note:** The information below is for reference purposes only. Please do not copy it verbatim into your report, including this warning.

## Summary Report: “Gen AI and Data Workshop”

### Event Objectives

- Share best practices in leveraging Generative AI and modern data architectures
- Introduce data-driven approaches and AI-assisted development workflows
- Provide guidance on selecting the right compute and data services for AI workloads
- Present AWS tools to support AI lifecycle and operationalization

### Speakers

- Jun Kai Loke – AI/ML Specialist SA, AWS
- Kien Nguyen – Solutions Architect, AWS
- Tamelly Lim – Storage Specialist SA, AWS
- Binh Tran – Senior Solutions Architect, AWS
- Taiki Dang – Solutions Architect, AWS
- Michael Armentano – Principal WW GTM Specialist, AWS

### Key Highlights

- Understanding the role of Generative AI in modern enterprises
- Building a unified, scalable, and governed data foundation to support AI workloads
- Combining GenAI with data engineering to automate complex tasks
- Optimizing AI-driven pipelines and query generation
- Governance and security for large-scale AI and data deployments
- Practical examples of AWS services such as S3, Glue, Redshift, Lake Formation, SageMaker, and Bedrock

#### Generative AI and Data Modernization Pillars

- **Data Pipeline Management:** Automate and monitor AI-ready pipelines
- **Caching and Performance Strategy:** Optimize data access and processing
- **Integration Handling:** Flexible communication between AI and data services

#### Generative AI Development Approaches

- **Four-step AI integration:** Identify AI use cases → Design data flow → Define automation → Deploy models
- **Case studies:** Real-world examples of AI-assisted analytics and pipeline automation
- **Context mapping:** Patterns for integrating AI models with existing systems

#### Event-Driven and AI-Oriented Architecture

- **Integration patterns:** Pub/Sub, point-to-point, streaming
- **Benefits:** Loose coupling, scalability, and resilience
- **Sync vs async AI operations:** Trade-offs and best practices

#### Compute and Data Platform Evolution

- **Shared Responsibility Model:** EC2 → ECS → Fargate → Lambda → SageMaker
- **Serverless and AI benefits:** No server management, auto-scaling, pay-for-value
- **Functions vs Containers vs Managed AI Services:** Criteria for appropriate choice

#### AWS AI Tools

- **Amazon Q Developer:** SDLC automation for AI projects
- **Code transformation:** Modernizing legacy analytics code and pipelines
- **AWS Transform agents:** Integrate AI into existing workloads (VMware, Mainframe, .NET)

### Key Takeaways

**Design Mindset**

- Business-first approach: Focus on business outcomes, not just technology
- Shared vocabulary: Importance of communication between business and tech teams
- Bounded contexts: Managing complexity in AI-enabled systems

**Technical Architecture**

- Event and AI-driven workflow modeling techniques
- Use event-driven communication instead of synchronous calls where possible
- Integration patterns: When to use sync, async, pub/sub, streaming
- Compute spectrum: Criteria for choosing VM, containers, serverless, or AI-managed services

**Modernization Strategy**

- Phased approach: Follow a clear roadmap for AI adoption
- 7Rs framework: Multiple paths for modernizing applications and data platforms
- ROI measurement: Evaluate business value from AI insights and operational improvements

### Applying to Work

- Apply AI-assisted workflows to current projects
- Refactor pipelines and data architectures to support AI models
- Implement event-driven patterns for AI operations
- Adopt serverless or managed AI services: Pilot AWS Lambda, SageMaker, and Bedrock
- Integrate Amazon Q Developer into SDLC to boost AI productivity

### Event Experience

Attending the “Gen AI and Data Workshop” was extremely valuable, providing a comprehensive view of modernizing data architectures and leveraging AI for operational efficiency. Key experiences included:

**Learning from highly skilled speakers**

- Experts from AWS shared best practices in AI adoption and data modernization
- Real-world case studies demonstrated practical AI use in enterprises

**Hands-on technical exposure**

- Participating in sessions helped visualize AI-driven workflows
- Learned how to design pipelines, define bounded contexts, and manage AI integration
- Understood trade-offs between synchronous and asynchronous operations in AI

**Leveraging modern tools**

- Explored Amazon Q Developer for SDLC support
- Learned to automate pipelines, code transformation, and pilot AI services

**Networking and discussions**

- Opportunity to exchange ideas with experts, peers, and business teams
- Reinforced the importance of a business-first approach rather than only focusing on technology

### Lessons Learned

- Applying AI-assisted patterns reduces operational complexity while improving scalability and resilience
- Phased implementation with ROI measurement is crucial for successful AI adoption
- AWS tools like Amazon Q Developer, SageMaker, and Bedrock can significantly boost productivity and shorten deployment cycles

### Some Event Photos

- Add your event photos here

**Overall**, the event provided both technical knowledge and practical insights, reshaping perspectives on AI-driven data architectures, system modernization, and cross-team collaboration.
