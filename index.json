[{"uri":"https://nhutruong47.github.io/aws/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"AWS for Industries: Equinox Builds a Scalable, Repeatable Architecture Using AWS IoT Services by Girish Nazhiyath on 07 JUL 2025 Categories: Amazon Data Firehose, Amazon SageMaker AI, Amazon Simple Storage Service (S3), AWS IoT Core, AWS IoT Greengrass, Industries, Kinesis Data Streams, Retail\nWith over 100 clubs globally, premium fitness company Equinox requires scalable, repeatable infrastructure that can support its integrated digital and physical experiences. Its connected experiences, which include competitive cycling classes, are a key offering that motivates its members to track and meet their fitness goals in a mobile app.\nHowever, delivering fitness insights in near real time was a challenge for the company. When it first developed its connected experiences, Equinox ran its workloads on a Windows-based server. Each service ran as an individual task. This meant that Equinox‚Äôs engineers had to broker updates for each service, which was time-consuming and led to inconsistencies in software updates.\n‚ÄúIt was a nightmare,‚Äù says Sindhura Nallapareddy, senior director of engineering at Equinox. ‚ÄúIt wasn‚Äôt just one component that we had to manage. We had to check on our servers, databases, and the message broker installations every time we made an update.‚Äù\nEquinox wanted to streamline software updates for its engineers and support near real-time data streaming. So, the company turned to Amazon Web Services (AWS) to rearchitect its stack.\n‚ÄúWe did not want to be held back with operational challenges,‚Äù says Nallapareddy. ‚ÄúWe wanted to focus on the possibilities that we could create, including fitness challenges in our studios and across our clubs.‚Äù\nConnecting Its In-Club Devices to the Cloud Using AWS IoT Core Equinox evaluated cloud-based solutions to support near real-time data processing and streamline engineering workflows. Prioritizing for stability and ease of use, Equinox tested AWS IoT Core, which easily and securely connect devices to the cloud.\nTo gather data from its bikes, Equinox uses VAST Internet of Things (IoT) devices that are installed on its workout equipment. These devices followed MPU protocol. Before adopting AWS IoT Core, the company gathered data from its bikes in user datagram protocol (UDP) packets, which were stored on PCs connected to local networks.\nAfter collection, data was processed using custom actions and jobs. These jobs would run for several hours before delivering fitness insights to the mobile app for members.\n‚ÄúThe proof of concept that we created was to see if we could sync data from our edge servers to AWS and if we could reliably establish cloud connectivity to support more seamless deployments,‚Äù says Aneesh Pillai, engineer at Equinox.\nUsing AWS IoT core, the company can collect UDP packets from its bikes using MQTT protocol, which then transmits data into its edge servers and Amazon Simple Storage Service (Amazon S3), an object storage service built to retrieve any amount of data from anywhere.\nBy storing its data on edge devices and in the cloud, Equinox has reduced the likelihood of data loss, improving overall data quality.\n‚ÄúWe appreciate the reliability of the data that is sent between our edge servers and AWS,‚Äù says Pillai. ‚ÄúThe speed and ease of managing these components are top-tier benefits for me.‚Äù\nMaking Unified Software Deployments with AWS IoT Greengrass Streamlining software deployments was essential to helping engineers focus on building more personalized member experiences. So, Equinox built a software management application for its engineering team using AWS IoT Greengrass, which is an open-source edge runtime and cloud service for building, deploying, and managing device software.\nTo start, Equinox deployed a single AWS IoT Greengrass instance onto one of its edge servers. After stabilizing, the Equinox team replicated this setup across all its edge servers. Previously, Equinox collected cycling data from 48 of its clubs; using AWS IoT Greengrass, the company was able to onboard 80 clubs in a matter of months.\nWhen a software update is available, Equinox can trigger deployments while classes are occurring without experiencing data loss. Additionally, the company can initiate deployments in a single club or across all its locations.\n‚ÄúThat has been an important improvement for us to overcome the operational challenges we dealt with before,‚Äù says Nallapareddy. ‚ÄúAWS IoT Greengrass has alleviated our teams by sending updates in a bundled package.‚Äù\nSecuring Data Streaming in the Cloud Equinox implemented Amazon Data Firehose to reliably load near real-time streams into data lakes, warehouses, and analytics services. To stream its data into the cloud, Equinox‚Äôs engineers created two sets of rules for collecting data from IoT devices using Amazon Kinesis Data Streams, which can easily stream data at any scale.\nOne stream collects heartbeats and biometrics. The other stream transmits data related to bike usage, such as battery power. Data is synced into the cloud on an event basis in near real time. The company also added additional logic to its solution to calculate the number of calories burned and miles cycled, which it provides to its members through its mobile app.\n‚ÄúAmazon Kinesis Data Streams is absolutely amazing,‚Äù says Pillai. ‚ÄúI can look right now and see all the data that‚Äôs available and which equipment is being used. It‚Äôs in near real time, and there‚Äôs no data latency or loss.‚Äù\nTo avoid duplicating data from its bikes, Equinox implemented data filtering. The engineering team also set up a second, private network for transmitting UDP packets from its bikes and other workout equipment, helping keep its members‚Äô data secure.\nBoosting Engineering Productivity and Experimenting with AI/ML With a scalable, repeatable stack running on AWS, Equinox has freed its engineers from complex deployments and integrations. Using a custom script, Equinox can scale its IoT solution to clubs in minutes, and its engineers are saving on deployments every month.\n‚ÄúWe can focus on the business instead of operational challenges,‚Äù says Pillai. ‚ÄúWe don‚Äôt have to worry about the connections between the cloud and local computers. Seamless deployments are one of the biggest benefits of our architecture.‚Äù\nThe automated setup has empowered Equinox‚Äôs engineers to focus on value-added tasks, including experimenting with artificial intelligence and machine learning. Its teams plan to use Amazon SageMaker AI to build, train, and deploy machine learning models‚Äîincluding foundation models‚Äîfor any use case with fully managed infrastructure, tools, and workflows.\nEquinox‚Äôs engineering team is looking forward to building more personalized member offerings and predictive maintenance recommendations for its workout equipment.\n‚ÄúThe confidence that we have in our data has unlocked so much potential,‚Äù says Eswar Veluri, executive vice president and chief technology officer at Equinox. ‚ÄúWe can explore club-to-club challenges, cycling milestones, and individual competitions. There is a lot of excitement across the company to uncover and unlock even more experiences.‚Äù\nGirish Nazhiyath Girish Nazhiyath currently serves as a Senior Solutions Architect at AWS. In his current role, Girish provides retail industry and technology expertise to retailers across the US to enable them to leverage best-of-breed AWS technologies and practices. Prior to working at AWS, Girish has over 25 years of retail career experience with global retail and technology companies like Microsoft, IBM, NEC, Fujitsu/ICL, Csoft International, Marconi Commerce Systems and Gilbarco Veeder-Root focusing on store systems, omnichannel, customer experience, biometric payments, and retail innovation. He has worked with retailers within the convenience store, grocery, telecom, big box/department and specialty retail verticals throughout his career and his focus areas include both brick-and-mortar and online Point-Of-Service and corporate/back-office systems.\n"},{"uri":"https://nhutruong47.github.io/aws/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/","title":"Create a gateway endpoint","tags":[],"description":"","content":" Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "},{"uri":"https://nhutruong47.github.io/aws/4-eventparticipated/4.1-event1/","title":"Event 1: Vietnam Cloud Day 2025 - Keynote Address","tags":[],"description":"","content":"Summary Report: ‚ÄúVietnam Cloud Day 2025: Keynote Address‚Äù Date and Topic Date and Time: Thursday, September 18, 2025 (9:00 AM ‚Äì 12:00 PM) Topic: Keynote Address \u0026amp; Panel Discussion: Navigating the GenAI Revolution Event Objectives To deliver strategic direction from the Government, AWS, and large enterprises regarding the vision for Cloud and AI in Vietnam. To share the experiences and challenges faced by Executive leaders in adopting and managing the organizational changes brought by Generative AI (GenAI). To provide an overview of the role of Cloud technology in driving business innovation. Speakers/Panelists Eric Yeo - Country General Manager, Vietnam, Cambodia, Laos \u0026amp; Myanmar, AWS (Keynote) Jaime Valles - Vice President, General Manager Asia Pacific and Japan, AWS (AWS Keynote) Dr. Jens Lottner - CEO, Techcombank (Customer Keynote 1) Ms. Trang Phung - CEO \u0026amp; Co-Founder, U2U Network (Customer Keynote 2) Jeff Johnson - Managing Director, ASEAN, AWS (Moderator Panel) Vu Van - Co-founder \u0026amp; CEO, ELSA Corp (Panelist) Nguyen Hoa Binh - Chairman, Nexttech Group (Panelist) Dieter Botha - CEO, TymeX (Panelist) Highlights 1. Keynote Address \u0026amp; Customer Keynotes National \u0026amp; AWS Vision: Listening to important speeches from government representatives and senior AWS leaders on the role of cloud computing and AI in digital economic growth. Enterprise Case Studies: Learning from the digital transformation experiences and new technology adoption by large enterprises such as Techcombank and U2U Network. 2. Panel discussion: Navigating the GenAI Revolution: Strategies for Executive Leadership GenAI Strategy: Leaders discussed how to steer organizations through the rapid advancements of GenAI. Innovation Culture: Sharing insights on fostering an innovation culture, aligning AI initiatives with business objectives, and managing organizational change associated with AI integration. Key Takeaways Leadership and Strategy Mindset GenAI is the Operational Core: GenAI is not merely a tool for the technical team but a strategic executive issue, requiring top-to-bottom alignment with business objectives. Learning from Leaders: Gained valuable perspectives from CEOs on how they balance the pursuit of new technology (AI) with maintaining core business stability and growth. Expanding Network and Vision Holistic View: Gained a comprehensive view of Cloud and AI technology trends at the national and regional levels. Understanding Challenges: Understood the practical challenges (governance, culture, investment) that large organizations face when implementing new technologies. Application to Work Technology Alignment: Applying the learned strategic mindset to align AWS tools and services (such as Bedrock/SageMaker) with specific project/company business goals. Value Proposition: Using data and case studies from the Keynote to create clearer value propositions when discussing Cloud technology with relevant stakeholders. Event Experience Attending the Keynote session of Vietnam Cloud Day 2025 provided a unique experience in terms of scale and strategic level.\nInspirational Environment: Witnessing top technology leaders in Vietnam and the region directly discuss the future of AI. Informational Value: The Keynotes, particularly those from the CEOs, provided realistic perspectives and lessons on leadership capabilities in a rapidly changing technological environment. Overall, this event helped me elevate my strategic vision and understand the complexity of integrating AI into core business decisions. Event image !\n"},{"uri":"https://nhutruong47.github.io/aws/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Truong Thai Nhu\nPhone Number: 0328416716\nEmail: nhuttse180082@fpt.edu.vn\nUniversity: FPT University, Ho Chi Minh City\nMajor: Software Engineering\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 12/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://nhutruong47.github.io/aws/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "},{"uri":"https://nhutruong47.github.io/aws/5-workshop/5.4-s3-onprem/5.4.1-prepare/","title":"Prepare the environment","tags":[],"description":"","content":"To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "},{"uri":"https://nhutruong47.github.io/aws/1-worklog/","title":"Worklog","tags":[],"description":"","content":"On this page, how will you introduce your worklog? In how many weeks did you complete the program? What did you do during those weeks?\nTypically and as a standard, a worklog is carried out over about 3 months (during the internship period) with the weekly content as follows:\nWeek 1: Getting started with AWS and basic AWS services\nWeek 2: Deploying EC2, practicing IAM Roles and advanced AWS CLI\nWeek 3: Mastering VPC, Subnet, Internet Gateway, and SG/NACL security\nWeek 4: Leveraging Amazon S3: Storage Classes, Versioning, and Static Website Hosting\nWeek 5: Deploying Amazon RDS, configuring Multi-AZ and Read Replicas\nWeek 6: Working with DynamoDB (NoSQL) and ElastiCache (Caching Layer)\nWeek 7: Configuring Auto Scaling Group (ASG), ALB, and monitoring with CloudWatch\nWeek 8: Building Serverless architecture with AWS Lambda and API Gateway\nWeek 9: Processing Data Streaming with Amazon Kinesis and ETL process with AWS Glue\nWeek 10: Integrating asynchronous applications using SQS, SNS, and Step Functions\nWeek 11: Deploying advanced security: AWS WAF, Shield, and GuardDuty\nWeek 12: Summarizing all knowledge, cleaning up resources, and certification orientation\n"},{"uri":"https://nhutruong47.github.io/aws/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Objectives for Week 1: Connect and get familiar with members of the First Cloud Journey. Understand basic AWS services, how to create and manage costs with an AWS account. Learn how to use the console \u0026amp; CLI to interact with and manage services. Tasks to be implemented this week: Day Task Start Date Completion Date Resources 2 - Get to know FCJ members - Read and note the rules and regulations at the internship unit 08/09/2025 08/09/2025 3 - Learn about AWS and basic service types + Compute (EC2) + Storage (S3) + Networking (VPC) + Database (RDS) 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Identity and access management + Install AWS CLI \u0026amp; configuration + Use AWS CLI for basic operations 10/09/2025 10/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn effective cost management with AWS budgets + Budget + Cost Budget, + Usage Budget + Reservation (RI) Budget + Savings plans Budget - Practice: + Create Cost Budget + Create Usage Budget + Create RI Budget + Create Savings Plans Budget + Clean up resources 11/09/2025 11/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn about AWS Support service - AWS Support Plans + Basic, Developer, Business, and Enterprise Plans - Types of support requests + Account and Billing Support + Service Limit Increase Support + Technical Support - Practice: + Select Basic support plan + Create a support request 12/09/2025 12/09/2025 https://cloudjourney.awsstudygroup.com/ Results achieved in Week 1: Understood what AWS is and grasped the basic service groups:\nCompute: Provides processing resources for applications like virtual machines, containers, etc. Storage: Used for data storage, backup, and recovery. Networking: Manages network infrastructure, security, and connections between AWS resources. Database: Provides relational and non-relational database management services. Successfully created and configured an AWS Free Tier account identity.\nLearned how to create and manage User Groups and Users.\nLearned how to log in using IAM and understood that users in the same group share granted permissions.\nFamiliarized with the AWS Management Console and learned how to search, access, and use services from the web interface.\nInstalled and configured AWS CLI on the computer including:\nAccess Key Secret Key Default Region Used AWS CLI to perform basic operations such as:\nCheck account info \u0026amp; configuration List regions Create and delete S3 Buckets Use Amazon SNS Create IAM groups, users, and add users Create and delete access keys Create and perform basic configuration for VPC Launch and terminate EC2 Grasped how to manage and monitor costs on AWS through tools:\nCreate and configure Budget plans (Cost, Usage, RI, Savings Plan). Learned how to clean up resources to manage costs effectively. Understood AWS support plans and learned how to create support requests from the support center.\nBasic: Free, supports account and billing issues from the help center. Developer: $29/month, basic architecture advice, and unlimited technical support created from the root user account. Business: $100/month, a popular choice for SMEs with support such as: Specific Use-case guidance, AWS Support API assistance, unlimited support requests created by all IAM Users, etc. Enterprise: $15,000/month, for large-scale enterprises guaranteed with standard and strictest security criteria with security services like: software architecture, infrastructure, comprehensive strategy and cost optimization support, priority care for support requests, etc.\nFamiliarized with the AWS Console interface and proficient in basic operations via both Console and CLI.\n"},{"uri":"https://nhutruong47.github.io/aws/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"AWS Public Sector Blog: AWS expands its support of ARPA-H Sprint for Women‚Äôs Health performers by Dr. Dawn Heisey-Grove, Kat (Darula) Esser, MID, and Lauren Hollis | on 10 JUN 2025 Categories: Announcements, Artificial Intelligence, AWS HealthOmics, Healthcare, Internet of Things, Life Sciences, Public Sector\nAmazon Web Services (AWS) provides innovative health researchers with the agility, adaptivity, and resilience they need to accelerate their research. Because of this, AWS expanded the scope of its commitment to the Advanced Research Projects Agency for Health (ARPA-H) Sprint for Women‚Äôs Health performers to include its Spark funding track awardees. These awardees focus on transformative early-stage research in women‚Äôs health.\nThese Spark awardees will receive AWS credits funded by AWS Social Responsibility \u0026amp; Impact and facilitated through the ARPA-H Investor Catalyst Hub, and technical support to guide them along their journey.\n‚ÄúWe‚Äôre excited for this collaboration with AWS to support our early-stage researchers in accelerating their commercialization journey,‚Äù stated Chelsea Schiller, Investor Catalyst Hub director.\nLet‚Äôs take a look at some of the innovative research projects these awardees are pursuing:\nThe California Institute of Technology (CalTech) is developing a wearable sweat sensing system for objective chronic pain assessment. Their solution will simultaneously monitor multiple types of pain biomarkers in sweat. CalTech researchers will combine those biomarkers with vital sign data and create machine learning models that predict pain levels in real-time. Dr. Wei Gao, professor of medical engineering at CalTech, predicts that ‚Äúreal-time objective pain measurement will transform how the medical community treats patients, removing some of the guess work out of pain management.‚Äù\nNura Health is also focused on biomarkers and precision medicine. Endometriosis diagnoses can take an average of 5-10 years to diagnose due to requiring an invasive surgical procedure for confirmation. Nura Health hopes to reduce that timeline to just days‚Äîand eliminate the need for surgical confirmation‚Äîby launching a platform that measures unique biomarkers found in blood while simultaneously analyzing genetic profiles in order to recommend precise, optimal treatments. ‚ÄúOur solution will bring faster diagnosis and a personalized treatment plan for women, significantly revamping the path to disease detection and symptom alleviation,‚Äù Varun Kapoor, co-founder and CEO of Nura Health, stated.\nAncilia Biosciences is initially applying its unique technology to develop a novel approach to treating bacterial vaginosis, a common, recurring condition that is associated with preterm birth, sexually transmitted infections, and infertility. ‚ÄúOur goal is to apply our advanced analytic and biological tools to unlock the vast potential of an entirely new class of therapies with major applications in both women‚Äôs health and broader indications,‚Äù declared Dr. Alexandra Sakatos, co-founder and CEO at Ancilia Biosciences.\nGeneral Proximity, focused on transforming cancer treatment to a single drug, describes their solution as ‚Äúpioneering proximity medicine in the battle against women‚Äôs cancer.‚Äù Their solution uses Induced Proximity Medicines to target a protein prevalent in breast and gynecological cancers.\nThese awardees will use AWS purpose-built health services designed to accelerate innovation. AWS provides the most comprehensive set of Internet of Things, artificial intelligence, and purpose-built health services designed to handle complex health data. These awardees will leverage AWS HealthOmics to transform their genomic, transcriptomic, and other omics data into insight. They can gain insights faster, supporting their efforts to move from idea to market with virtually unlimited compute capacity with the complete suite of high performance computing (HPC) products and services on AWS.\nThe agility and scope of AWS services means health researchers have the flexibility they need to explore their research questions. Learn more about AWS health and life sciences services, and see how scientists leveraged AWS HPC tools to unlock disease insights. Then contact us for help in accelerating your research.\nAcknowledgement: This research was, in part, funded by the Advanced Research Projects Agency for Health (ARPA-H). The views and conclusions contained in this blog post are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the United States Government.\nTAGS: announcement, Artificial Intelligence, AWS healthcare, AWS Public Sector, Internet of Things, life sciences\nAuthors Dr. Dawn Heisey-Grove\nDawn is a federal public health leader on the U.S. Federal Civilian team. She has spent her career finding new ways to use existing or new data to modernize public health surveillance and research. With a background in public health and informatics, Dr. Heisey-Grove leads innovation and modernization in public health agencies, supporting infectious and chronic disease, bioinformatics, environmental health, and more.\nKat (Darula) Esser, MID\nKat has over 25 years of experience leading the development and execution of social innovation portfolios that drive organizational growth for global public and private sector health systems and organizations. Currently she serves as impact initiatives leader for the AWS Social Impact \u0026amp; Responsibility team. In this role, Kat oversees the impact initiatives portfolio that uses cloud technology to address the most pressing global health and education issues. As a key part of this work, Kat oversaw the first of its kind AWS Health Equity X Cloud Technology Landscape Analysis and the AWS Food Value Chain x Cloud Technology Landscape Analysis that synthesizes current state, gaps and future opportunities at the intersection of health equity, nutrition security and cloud technology.\nLauren Hollis\nLauren is a program manager for AWS Social Responsibility and Impact. She leverages her background in economics, healthcare research, and technology to support mission-driven organizations deliver social impact using AWS cloud technology. In her free time, Lauren enjoys reading and playing the piano and cello.\n"},{"uri":"https://nhutruong47.github.io/aws/2-proposal/","title":"B·∫£n ƒë·ªÅ xu·∫•t","tags":[],"description":"","content":"Proposal ‚Äì Smart Resume Analyzer A Unified AWS Serverless solution to analyze CVs vs JDs and generate Fit Scores\nNote: This proposal follows the sectioning style of your previous _index.md sample but is rewritten for the Smart Resume Analyzer project.\n1) Executive Summary Smart Resume Analyzer is a serverless web platform that evaluates the match between a candidate‚Äôs CV and a Job Description (JD). It calculates a Fit Score, detects skill gaps, and provides personalized learning suggestions.\nThe solution is implemented by a 5‚Äëmember team in 4 weeks on AWS using managed, pay‚Äëas‚Äëyou‚Äëgo services to keep costs near zero for a demo workload. The UI is built with Next.js and hosted on AWS Amplify; the backend uses API Gateway + Lambda with DynamoDB, S3, Comprehend, Textract, and Cognito.\nKey outcomes\n90% faster CV screening for demo scenarios. Objective Fit Score with visual reports. Actionable learning roadmap per candidate. 2) Problem Statement 2.1 What‚Äôs the problem? Recruiters spend significant time manually reading CVs and comparing them to JDs. Candidates lack insight into which skills they are missing and how to improve. Existing tools are expensive or not tailored for Vietnamese/SEA use cases. 2.2 The solution Upload CV (PDF/DOCX) and JD ‚Üí automatic text extraction and NLP. Detect skills, experience, education; compute Fit Score vs JD. Recommend skill pathways mapped from a small SkillOntology store. Secure login with Cognito; results shown in a clean Next.js dashboard. 3) Solution Architecture (overview) Serverless, event‚Äëdriven architecture on AWS.\nMain components\nFrontend: Next.js UI (Amplify Hosting) for upload \u0026amp; result dashboard. API Layer: Amazon API Gateway ‚Üí AWS Lambda functions. Processing: parseResume ‚Üí Textract (if scanned PDF) ‚Üí normalized text. nlpAnalyze ‚Üí Comprehend ‚Üí entities/skills/phrases. recommendSkills ‚Üí compares to JD + SkillOntology in DynamoDB. Data: DynamoDB (results, ontology), S3 (temporary CV/JD). Identity: Cognito (JWT access tokens). Ops: IaC with AWS SAM, CI/CD via CodeBuild + CodePipeline, logging in CloudWatch. (A Mermaid architecture diagram is provided separately.)\n4) Technical Implementation 4.1 Tech stack Backend: .NET 8 (C# Minimal API on Lambda) Frontend: Next.js + TailwindCSS (Amplify Hosting) AWS: Lambda, API Gateway, DynamoDB, S3, Cognito, Comprehend, Textract IaC: AWS SAM CI/CD: CodeBuild + CodePipeline 4.2 End‚Äëto‚Äëend flow User authenticates via Cognito and obtains JWT. Frontend requests presigned URL to S3 ‚Üí uploads CV/JD. API Gateway invokes Lambda parseResume: If PDF scan ‚Üí Textract ‚Üí extract text; otherwise direct parse. Clean \u0026amp; normalize ‚Üí store interim artifacts on S3. Lambda nlpAnalyze uses Comprehend to detect entities/skills ‚Üí writes results to DynamoDB. Lambda recommendSkills loads SkillOntology from DynamoDB ‚Üí compares CV vs JD ‚Üí computes Fit Score + gaps. Frontend queries results via API ‚Üí renders charts/tables. 4.3 Data model (DynamoDB ‚Äì simplified) Table Profiles (PK: userId, SK: profileId) ‚Äì store latest CV parse. Table Analyses (PK: analysisId) ‚Äì fit score, gaps, timestamps. Table SkillOntology (PK: skillId, attributes: name, tags, learningPath[]). 4.4 API (high level) POST /upload-url ‚Üí presign for CV/JD. POST /analyze ‚Üí triggers pipeline for a given S3 key pair. GET /analyses/{id} ‚Üí returns Fit Score \u0026amp; recommendations. GET /skills/{id} ‚Üí (optional) fetch a skill‚Äôs learning path. 5) Timeline \u0026amp; Milestones (4 weeks) Week Milestone Deliverables 1 Foundation SAM template, DynamoDB tables, Cognito, base UI 2 Parsing \u0026amp; NLP parseResume, nlpAnalyze, JD parsing, unit tests 3 Recommender \u0026amp; FE integration recommendSkills, dashboard, charts 4 Demo \u0026amp; hardening E2E tests, logging, cost tuning, slide deck 6) Budget Estimation (demo scale) Indicative, assuming \u0026lt; 500 requests/month\nLambda: ~$0.02 API Gateway: ~$0.01 S3 (few GB, low requests): ~$0.10 DynamoDB (on‚Äëdemand, low R/W): ~$0.05 Amplify Hosting: ~$0.30 Comprehend + Textract (small pages): ~$0.40 Cognito: $0.00\nTotal ‚âà $0.9 / month (~$10 / year) 7) Security, Risks \u0026amp; Mitigations Security\nPrivate S3 buckets with SSE‚ÄëKMS; presigned uploads only. IAM least privilege; API protected by Cognito JWT. PII masking for logs; CloudWatch alarms. Optional: set lifecycle rules to delete raw CV/JD after analysis. Risks \u0026amp; mitigations\nNLP accuracy: Provide supported formats + fallback to keyword rules. Large/unclean CVs: Validate size/format; sanitize before NLP. Cost spikes: AWS Budget alarms; cap page counts per request. 8) Expected Outcomes Automated CV‚ÄëJD matching with transparent Fit Score. Visual breakdown of skills matched vs gaps and learning roadmap. Serverless, low‚Äëops stack that‚Äôs easy to demo, extend, and localize. üìÑ Proposal Document (Google Docs) üëâ Review Proposal here:\nGOOGLE DOC LINK\n"},{"uri":"https://nhutruong47.github.io/aws/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/","title":"Create an S3 Interface endpoint","tags":[],"description":"","content":"In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"},{"uri":"https://nhutruong47.github.io/aws/4-eventparticipated/4.2-event2/","title":"Event 2: Vietnam Cloud Day 2025 - Track for Builders","tags":[],"description":"","content":"Summary Report: ‚ÄúVietnam Cloud Day 2025: Track for Builders‚Äù Date and Topic Date and Time: Thursday, September 18, 2025 (1:00 PM ‚Äì 5:00 PM) Topic: Track B: Gen AI and Data Event Objectives To share strategies and best practices for building a unified, scalable Data Foundation on AWS. To introduce the strategic roadmap and trends for applying Generative AI (GenAI) within organizations. To analyze the role of AI in the Development Lifecycle (AI-DLC) to improve software speed and quality. To guide methods and services for securing GenAI applications across all layers (Infrastructure, Models, Applications). Speakers Jun Kai Loke - AI/ML Specialist SA, AWS Kien Nguyen - Solutions Architect, AWS Tamelly Lim - Storage Specialist SA, AWS Binh Tran - Senior Solutions Architect, AWS Taiki Dang - Solutions Architect, AWS Michael Armentano - Principal WW GTM Specialist, AWS Highlights 1. Building a Unified Data Foundation on AWS for AI and Analytics Workloads Unified Data Foundation: Strategies for constructing a flexible, scalable data infrastructure, focusing on Data Ingestion, Storage, Processing, and Data Governance. 2. Building the Future: Gen AI Adoption and Roadmap on AWS Vision and Trends: Sharing AWS\u0026rsquo;s comprehensive vision, emerging trends, and strategic roadmap for the adoption of Generative AI and new GenAI services. 3. AI-Driven Development Lifecycle (AI-DLC) Shaping the future of Software Implementation SDLC Transformation: Redefining the software development process by embedding AI as a central collaborator (AI-powered Execution) to improve speed and quality. 4. Securing Generative AI Applications with AWS: Fundamentals and Best Practices GenAI Security Challenges: Exploring unique security risks at each layer of the GenAI stack (Infrastructure, Models, Applications). Best Practices: Guidance on how AWS integrates security measures such as encryption, Zero-Trust architecture, and fine-grained access controls. 5. Beyond Automation: AI Agents as Your Ultimate Productivity Multipliers AI Agents: Discussion on the paradigm shift from basic automation to using AI Agents (intelligent partners) capable of self-learning, adapting, and executing complex tasks. Key Takeaways Data and AI Mindset Data Foundation is Essential: All AI/Analytics efforts must start with a unified, well-governed Data Governance foundation. AI-DLC Replaces Traditional SDLC: Learned how to embed AI into the software development lifecycle to increase speed and innovation. Technical Architecture and Security Multi-layer Security in GenAI: Security must extend beyond infrastructure to protect both the model and the application flow using measures like encryption and Zero-Trust. AI Agents - New Level of Automation: Understood the potential of AI Agents in automating complex processes and exponentially multiplying business productivity. Application to Work Enhancing Data Governance: Applying learned data governance methods to establish unified data storage, processing, and security principles for AI/ML projects. AI-DLC Experimentation: Researching and piloting the integration of AI tools (such as Amazon Q) into the development process to automate repetitive tasks. GenAI Security Design: Ensuring that multi-layer security principles (data encryption, access control) are applied from the start when designing GenAI applications. Event Experience Participating in the Track for Builders was a highly valuable experience, focused entirely on data-driven and AI-centric architecture.\nFoundational and Strategic Knowledge: I gained access not only to specific AWS services but also to the overall strategy for building a data foundation for AI and the GenAI roadmap. Vision for Software Development: The discussion on AI-DLC changed my perspective on SDLC, placing AI at the center of software creation. "},{"uri":"https://nhutruong47.github.io/aws/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:AllocateAddress\u0026#34;, \u0026#34;ec2:AssociateAddress\u0026#34;, \u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;, \u0026#34;ec2:AssociateRouteTable\u0026#34;, \u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;, \u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;, \u0026#34;ec2:AttachInternetGateway\u0026#34;, \u0026#34;ec2:AttachNetworkInterface\u0026#34;, \u0026#34;ec2:AttachVolume\u0026#34;, \u0026#34;ec2:AttachVpnGateway\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;, \u0026#34;ec2:CreateClientVpnRoute\u0026#34;, \u0026#34;ec2:CreateCustomerGateway\u0026#34;, \u0026#34;ec2:CreateDhcpOptions\u0026#34;, \u0026#34;ec2:CreateFlowLogs\u0026#34;, \u0026#34;ec2:CreateInternetGateway\u0026#34;, \u0026#34;ec2:CreateLaunchTemplate\u0026#34;, \u0026#34;ec2:CreateNetworkAcl\u0026#34;, \u0026#34;ec2:CreateNetworkInterface\u0026#34;, \u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:CreateRoute\u0026#34;, \u0026#34;ec2:CreateRouteTable\u0026#34;, \u0026#34;ec2:CreateSecurityGroup\u0026#34;, \u0026#34;ec2:CreateSubnet\u0026#34;, \u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateTransitGateway\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:CreateVpc\u0026#34;, \u0026#34;ec2:CreateVpcEndpoint\u0026#34;, \u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;, \u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;, \u0026#34;ec2:CreateVpnConnection\u0026#34;, \u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;, \u0026#34;ec2:CreateVpnGateway\u0026#34;, \u0026#34;ec2:DeleteCustomerGateway\u0026#34;, \u0026#34;ec2:DeleteFlowLogs\u0026#34;, \u0026#34;ec2:DeleteInternetGateway\u0026#34;, \u0026#34;ec2:DeleteNetworkInterface\u0026#34;, \u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:DeleteRoute\u0026#34;, \u0026#34;ec2:DeleteRouteTable\u0026#34;, \u0026#34;ec2:DeleteSecurityGroup\u0026#34;, \u0026#34;ec2:DeleteSubnet\u0026#34;, \u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;ec2:DeleteTransitGateway\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:DeleteVpc\u0026#34;, \u0026#34;ec2:DeleteVpcEndpoints\u0026#34;, \u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;, \u0026#34;ec2:DeleteVpnConnection\u0026#34;, \u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;ec2:DetachInternetGateway\u0026#34;, \u0026#34;ec2:DisassociateAddress\u0026#34;, \u0026#34;ec2:DisassociateRouteTable\u0026#34;, \u0026#34;ec2:GetLaunchTemplateData\u0026#34;, \u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34;, \u0026#34;ec2:ModifySecurityGroupRules\u0026#34;, \u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:ModifyVpcAttribute\u0026#34;, \u0026#34;ec2:ModifyVpcEndpoint\u0026#34;, \u0026#34;ec2:ReleaseAddress\u0026#34;, \u0026#34;ec2:ReplaceRoute\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:RunInstances\u0026#34;, \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;, \u0026#34;iam:AddRoleToInstanceProfile\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:CreateInstanceProfile\u0026#34;, \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteInstanceProfile\u0026#34;, \u0026#34;iam:DeletePolicy\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:GetInstanceProfile\u0026#34;, \u0026#34;iam:GetPolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListPolicyVersions\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:DeleteLayerVersion\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetLayerVersion\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:PublishLayerVersion\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;route53:ChangeTagsForResource\u0026#34;, \u0026#34;route53:CreateHealthCheck\u0026#34;, \u0026#34;route53:CreateHostedZone\u0026#34;, \u0026#34;route53:CreateTrafficPolicy\u0026#34;, \u0026#34;route53:DeleteHostedZone\u0026#34;, \u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;, \u0026#34;route53:GetHostedZone\u0026#34;, \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53domains:ListDomains\u0026#34;, \u0026#34;route53domains:ListOperations\u0026#34;, \u0026#34;route53domains:ListTagsForDomain\u0026#34;, \u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:AssociateResolverRule\u0026#34;, \u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:CreateResolverRule\u0026#34;, \u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;, \u0026#34;route53resolver:DeleteResolverRule\u0026#34;, \u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:DisassociateResolverRule\u0026#34;, \u0026#34;route53resolver:GetResolverEndpoint\u0026#34;, \u0026#34;route53resolver:GetResolverRule\u0026#34;, \u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;, \u0026#34;route53resolver:ListResolverEndpoints\u0026#34;, \u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;, \u0026#34;route53resolver:ListResolverRules\u0026#34;, \u0026#34;route53resolver:ListTagsForResource\u0026#34;, \u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:UpdateResolverRule\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketOwnershipControls\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketPolicyStatus\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:ListAccessPoints\u0026#34;, \u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;, \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:ListBucketMultipartUploads\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:ListJobs\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34;, \u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;, \u0026#34;s3:ListStorageLensConfigurations\u0026#34;, \u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:PutBucketAcl\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;, \u0026#34;secretsmanager:PutResourcePolicy\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;ssm:DescribeInstanceProperties\u0026#34;, \u0026#34;ssm:DescribeSessions\u0026#34;, \u0026#34;ssm:GetConnectionStatus\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;, \u0026#34;ssm:ListAssociations\u0026#34;, \u0026#34;ssm:ResumeSession\u0026#34;, \u0026#34;ssm:StartSession\u0026#34;, \u0026#34;ssm:TerminateSession\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "},{"uri":"https://nhutruong47.github.io/aws/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/","title":"Test the Gateway Endpoint","tags":[],"description":"","content":"Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"},{"uri":"https://nhutruong47.github.io/aws/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Objectives for Week 2: Clearly understand the role and operation of the EC2 (Elastic Compute Cloud) virtual server service. Master the process of launching, managing, and terminating an EC2 Instance. Master the concept and usage of IAM Roles to securely grant access permissions to EC2 resources. Use AWS CLI to perform basic EC2 management operations. Tasks to be implemented this week: Day Task Start Date Completion Date Resources 2 - Read and understand the core components of Amazon EC2 (AMI, Instance Type, Key Pair, Security Group). - Practice: Launch the first EC2 Instance (select Free Tier type). 15/09/2025 15/09/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn about the secure permission mechanism IAM Roles for EC2 (Instance Profiling). - Practice: + Create IAM Policy allowing s3:ListBucket. + Create an IAM Role and attach this Policy. + Attach the newly created Role to the EC2 Instance. 16/09/2025 16/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn EC2 service management commands with AWS CLI. - Practice: + Use CLI to view EC2 Instance information (describe-instances). + Use CLI to Stop/Start Instance. + Manage Security Groups via CLI. 17/09/2025 17/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Integration Practice: Verify IAM Role permissions. - Log in via SSH/Session Manager to the EC2 Instance. - Practice: Use the command aws s3 ls (CLI) from inside the EC2 to check if the Instance can read (list) S3 Buckets (proving the IAM Role is working). 18/09/2025 18/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Lifecycle \u0026amp; Cost Management: - Review EC2 lifecycle states (Pending, Running, Stopping, Terminated). - Practice: + Terminate the EC2 Instance. + Delete the created IAM Role and Policy to clean up resources. 19/09/2025 19/09/2025 https://cloudjourney.awsstudygroup.com/ Results achieved in Week 2: EC2: Grasped how to create, configure, and manage the lifecycle of EC2 virtual machines via Console and CLI. Understood the components that make up an EC2 instance. IAM Roles: Understood the advantages and usage of IAM Role (Instance Profile) to securely grant AWS service access permissions to EC2 resources, eliminating the need to store Access Keys on the server. Advanced AWS CLI: Learned how to use CLI commands to manage EC2 Instance states (Start, Stop, Describe) and perform remote operations. Integration Practice: Proved that the EC2 Instance can execute actions on S3 (e.g., aws s3 ls) thanks to permissions granted via IAM Role, confirming the security model works effectively. Resource Cleanup: Grasped the process of terminating EC2 Instances and cleaning up related IAM resources to ensure no costs are incurred. "},{"uri":"https://nhutruong47.github.io/aws/5-workshop/5.3-s3-vpc/","title":"Access S3 from VPC","tags":[],"description":"","content":"Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "},{"uri":"https://nhutruong47.github.io/aws/4-eventparticipated/4.3-event3/","title":"Event 3: AWS Cloud Mastery Series #1 - AI/ML/GenAI on AWS","tags":[],"description":"","content":"Summary Report: ‚ÄúAWS Cloud Mastery Series #1: AI/ML/GenAI on AWS‚Äù Date and Topic Date and Time: Saturday, November 15, 2025 (8:30 AM ‚Äì 12:00 PM) Topic: AI/ML/GenAI on AWS Event Objectives To provide an overview of AWS\u0026rsquo;s AI/ML services. To introduce Amazon SageMaker ‚Äì the end-to-end ML platform. To delve into Generative AI (GenAI) and Foundation Models via Amazon Bedrock. To instruct on Prompt Engineering techniques and the Retrieval-Augmented Generation (RAG) architecture. Speakers Detailed speaker information is not available in the original data, assumed to be AWS Vietnam Solution Architects/Specialists. Highlights 1. AWS AI/ML Services Overview Amazon SageMaker: The complete Machine Learning (ML) lifecycle, including data preparation, training, tuning, model deployment, and integrated MLOps. Live Demo: Hands-on walkthrough of SageMaker Studio. 2. Generative AI with Amazon Bedrock Foundation Models: Comparison and selection guidance for models like Claude, Llama, and Titan. Prompt Engineering: Advanced techniques such as Chain-of-Thought reasoning and Few-shot learning. Retrieval-Augmented Generation (RAG): Architecture and integration with a Knowledge Base to enhance accuracy and context for GenAI. Bedrock Agents \u0026amp; Guardrails: Building multi-step workflows and Guardrails (content moderation mechanisms) to ensure safety. Key Takeaways GenAI and ML Skills End-to-end ML Deployment: Gained proficiency in the fundamental steps for effectively building and deploying ML models on SageMaker. Bedrock Mastery: A clear understanding of Bedrock\u0026rsquo;s components and the capabilities of different Foundation Models. RAG Technique: Understood that the RAG architecture is a key factor for implementing GenAI into practical enterprise applications using private data. Application to Work Building GenAI Proof-of-Concepts (PoC): Using Amazon Bedrock to rapidly prototype GenAI use cases (e.g., document summarization, internal knowledge chatbots) with RAG techniques. Prompt Optimization: Applying the learned Prompt Engineering techniques to improve the quality of responses from Large Language Models. Event Experience In-depth Practicality: The Live Demos on SageMaker and Bedrock were highly valuable, providing a realistic view of service deployment. Focus on Detail: The event delved into technical aspects like model selection and optimization (Prompting), giving me a stronger foundation compared to purely introductory sessions. This event provided a solid foundation for me to begin working with Machine Learning and Generative AI projects on AWS.\n"},{"uri":"https://nhutruong47.github.io/aws/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/","title":"Test the Interface Endpoint","tags":[],"description":"","content":"Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "},{"uri":"https://nhutruong47.github.io/aws/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section will list and introduce the blogs you have translated. For example:\nBlog 1 - Equinox Builds a Scalable, Repeatable Architecture Using AWS IoT Services This blog details how the premium fitness company Equinox overcame challenges with its legacy infrastructure to provide near-real-time fitness insights. You will learn how Equinox uses AWS IoT Core to securely connect its in-club workout equipment to the cloud, AWS IoT Greengrass to streamline and scale software deployments across 80 clubs, and Amazon Kinesis Data Streams to process biometric and device data. The article highlights how this new architecture solves operational issues, improves data reliability, and frees up engineers to focus on innovation, such as future AI/ML projects using Amazon SageMaker.\nBlog 2 - AWS expands its support of ARPA-H Sprint for Women‚Äôs Health performers This blog announces that AWS is expanding its support for the ARPA-H Sprint for Women‚Äôs Health, specifically for its \u0026ldquo;Spark funding track\u0026rdquo; awardees. You will learn about the innovative, early-stage research being conducted, including projects from CalTech (a wearable sweat sensor for chronic pain assessment), Nura Health (rapid endometriosis diagnosis), Ancilia Biosciences (treating bacterial vaginosis), and General Proximity (cancer treatment). The article explains how these researchers will receive AWS credits and technical support, leveraging services like AWS HealthOmics and HPC (High-Performance Computing) to accelerate their work from idea to commercialization.\nBlog 3 - Supercharge your AI workflows by connecting to SageMaker Studio from Visual Studio Code This blog introduces a new capability that allows AI developers and ML engineers to connect their local Visual Studio Code (VS Code) environment directly to Amazon SageMaker Studio. You will learn about three key benefits: using a familiar IDE with scalable cloud compute resources, simplifying operational setup, and maintaining enterprise-grade security. The article provides a technical walkthrough, explaining prerequisites (like the AWS Toolkit and new IAM permissions) and the two connection methods (deep linking from Studio or via the AWS Toolkit). It also guides a practical use case: running a notebook in your local VS Code while using a remote kernel from a SageMaker Studio space.\n"},{"uri":"https://nhutruong47.github.io/aws/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Artificial Intelligence: Supercharge your AI workflows by connecting to SageMaker Studio from Visual Studio Code by Durga Sury, Raj Bagwe, Sri Aakash Mandavilli, and Edward Sun on 10 JUL 2025 Categories: Advanced (300), Amazon SageMaker Studio, Technical How-to\nAI developers and machine learning (ML) engineers can now use the capabilities of Amazon SageMaker Studio directly from their local Visual Studio Code (VS Code). With this capability, you can use your customized local VS Code setup, including AI-assisted development tools, custom extensions, and debugging tools while accessing compute resources and your data in SageMaker Studio. By accessing familiar model development features, data scientists can maintain their established workflows, preserve their productivity tools, and seamlessly develop, train, and deploy machine learning, deep learning and generative AI models.\nIn this post, we show you how to remotely connect your local VS Code to SageMaker Studio development environments to use your customized development environment while accessing Amazon SageMaker AI compute resources.\nThe local integrated development environment (IDE) connection capability delivers three key benefits for developers and data scientists:\nFamiliar development environment with scalable compute: Work in your familiar IDE environment while harnessing the purpose-built model development environment of SageMaker AI. Keep your preferred themes, shortcuts, extensions, productivity, and AI tools while accessing SageMaker AI features. Simplify operations: With a few clicks, you can minimize the complex configurations and administrative overhead of setting up remote access to SageMaker Studio spaces. The integration provides direct access to Studio spaces from your IDE. Enterprise grade security: Benefit from secure connections between your IDE and SageMaker AI through automatic credentials management and session maintenance. In addition, code execution remains within the controlled boundaries of SageMaker AI. This feature bridges the gap between local development preferences and cloud-based machine learning resources, so that teams can improve their productivity while using the features of Amazon SageMaker AI.\nSolution overview The following diagram showcases the interaction between your local IDE and SageMaker Studio spaces.\nThe solution architecture consists of three main components:\nLocal computer: Your development machine running VS Code with AWS Toolkit extension installed. SageMaker Studio: A unified, web-based ML development environment to seamlessly build, train, deploy, and manage machine learning and analytics workflows at scale using integrated AWS tools and secure, governed access to your data. AWS Systems Manager: A secure, scalable remote access and management service that enables seamless connectivity between your local VS Code and SageMaker Studio spaces to streamline ML development workflows. The connection flow supports two options:\nDirect launch (deep link): Users can initiate the connection directly from the SageMaker Studio web interface by choosing Open in VS Code, which automatically launches their local VS Code instance. AWS Toolkit connection: Users can connect through AWS Toolkit extension in VS Code by browsing available SageMaker Studio spaces and selecting their target environment. In addition to the preceding, users can also connect to their space directly from their IDE terminal using SSH. For instructions on connecting using SSH, refer to documentation here.\nAfter connecting, developers can:\nUse their custom VS Code extensions and tools Remotely access and use their space‚Äôs storage Run their AI and ML workloads in SageMaker compute environments Work with notebooks in their preferred IDE Maintain the same security parameters as the SageMaker Studio web environment Solution implementation Prerequisites To try the remote IDE connection, you must meet the following prerequisites:\nYou have access to a SageMaker Studio domain with connectivity to the internet. For domains set up in VPC-only mode, your domain should have a route out to the internet through a proxy, or a NAT gateway. If your domain is completely isolated from the internet, see Connect to VPC with subnets without internet access for setting up the remote connection. If you do not have a Studio domain, you can create one using the quick setup or custom setup option. You have permissions to update the SageMaker Studio domain or user execution role in AWS Identity and Access Management (IAM). You have the latest stable VS Code with Microsoft Remote SSH (version 0.74.0 or later), and AWS Toolkit extension (version v3.68.0 or later) installed on your local machine. Optionally, if you want to connect to SageMaker spaces directly from VS Code, you should be authenticated to access AWS resources using IAM or AWS IAM Identity Center credentials. See the administrator documentation for AWS Toolkit authentication support. You use compatible SageMaker Distribution images (2.7+ and 3.1+) for running SageMaker Studio spaces, or a custom image. If you‚Äôre initiating the connection from the IDE, you already have a user profile in the SageMaker Studio domain you want to connect to, and the spaces are already created using the Studio UI or through APIs. The AWS Toolkit does not allow creation or deletion of spaces. Set up necessary permissions We‚Äôve launched the StartSession API for remote IDE connectivity. Add the sagemaker:StartSession permission to your user‚Äôs role so that they can remotely connect to a space.\nFor the deep-linking experience, the user starts the remote session from the Studio UI. Hence, the domain default execution role, or the user‚Äôs execution role should allow the user to call the StartSession API. Modify the permissions on your domain or user execution role by adding the following policy statement:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;RestrictStartSessionOnSpacesToUserProfile\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;sagemaker:StartSession\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;arn:*:sagemaker:${aws:Region}:${aws:AccountId}:space/${sagemaker:DomainId}/*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;ArnLike\u0026#34;: { \u0026#34;sagemaker:ResourceTag/sagemaker:user-profile-arn\u0026#34;: \u0026#34;arn:*:sagemaker:${aws:Region}:${aws:AccountId}:user-profile/${sagemaker:DomainId}/${sagemaker:UserProfileName}\u0026#34; } } } ] } If you‚Äôre initializing the connection to SageMaker Studio spaces directly from VS Code, your AWS credentials should allow the user to list the spaces, start or stop a space, and initiate a connection to a running space. Make sure that your AWS credentials allow the following API actions:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;sagemaker:ListSpaces\u0026#34;, \u0026#34;sagemaker:DescribeSpace\u0026#34;, \u0026#34;sagemaker:UpdateSpace\u0026#34;, \u0026#34;sagemaker:ListApps\u0026#34;, \u0026#34;sagemaker:CreateApp\u0026#34;, \u0026#34;sagemaker:DeleteApp\u0026#34;, \u0026#34;sagemaker:DescribeApp\u0026#34;, \u0026#34;sagemaker:StartSession\u0026#34;, \u0026#34;sagemaker:DescribeDomain\u0026#34;, \u0026#34;sagemaker:AddTags\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } This initial IAM policy provides a quick-start foundation for testing SageMaker features. Organizations can implement more granular access controls using resource Amazon Resource Name (ARN) constraints or attribute-based access control (ABAC). With the introduction of the StartSession API, you can restrict access by defining space ARNs in the resource section or implementing condition tags according to your specific security needs, as shown in the following example.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowRemoteAccessByTag\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;sagemaker:StartSession\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;aws:ResourceTag/User\u0026#34;: \u0026#34;\u0026lt;user-identifier\u0026gt;\u0026#34; } } } ] } Enable remote connectivity and launch VS Code from SageMaker Studio To connect to a SageMaker space remotely, the space must have remote access enabled. Before running a space on the Studio UI, you can toggle Remote access on to enable the feature, as shown in the following screenshot.\nAfter the feature is enabled, choose Run space to start the space. After the space is running, choose Open in VS Code to launch VS Code.\nThe first time you choose this option, you‚Äôll be prompted by your browser to confirm opening VS Code. Select the checkbox Always allow studio to confirm and then choose Open Visual Studio Code.\nThis will open VS Code, and you will be prompted to update your SSH configuration. Choose Update SSH config to complete the connection. This is also a one-time setup, and you will not be prompted for future connections.\nOn successful connection, a new window launches that is connected to the SageMaker Studio space and has access to the Studio space‚Äôs storage.\nConnect to the space from VS Code Using the AWS Toolkit, you can list the spaces, start, connect to a space, or connect to a running space that has remote connection enabled. If a running space doesn‚Äôt have remote connectivity enabled, you can stop the space from the AWS Toolkit and then select the Connect icon to automatically turn on remote connectivity and start the space. The following section describes the experience in detail.\nAfter you‚Äôre authenticated into AWS, from AWS Toolkit, access the AWS Region where your SageMaker Studio domain is. You will now see a SageMaker AI section. Choose the SageMaker AI section to list the spaces in your Region. If you‚Äôre connected using IAM, the toolkit lists the spaces across domains and users in your Region. See the [Optional] Filter spaces to a specific domain or user below on instructions to view spaces for a particular user profile. For Identity Center users, the list is already filtered to display only the spaces owned by you.\nAfter you identify the space, choose the connectivity icon as shown in the screenshot below to connect to the space.\nOptional: Filter spaces to a specific domain or user\nWhen connecting to an account using IAM, you will see a list of spaces in the account and region. This can be overwhelming if the account has tens or hundreds of domains, users and spaces. The toolkit provides a filter utility that helps you quickly filter the list of spaces to a specific user profile or a list of user profiles. Next to SageMaker AI, choose the filter icon as shown in the following screenshot.\nYou will now see a list of user profiles and domains. Scroll through the list or enter user profile or domain name, and then select or unselect to filter the list of spaces by domain or user profile.\nUse cases Following use cases demonstrate how AI developers and machine learning (ML) engineers can use local integrated development environment (IDE) connection capability.\nConnecting to a notebook kernel After you‚Äôre connected to the space, you can start creating and running notebooks and scripts right from your local development environment. By using this method, you can use the managed infrastructure provided by SageMaker for resource-intensive AI tasks while coding in a familiar environment. You can run notebook cells on your SageMaker Distribution or custom image kernels, and can choose the IDE that maximizes your productivity. Use the following steps to create and connect your notebook to a remote kernel ‚Äì\nOn your VS Code file explorer, choose the plus (+) icon to create a new file, name it remote-kernel.ipynb. Open the notebook and run a cell (for example, print (\u0026ldquo;Hello from remote IDE\u0026rdquo;). VS Code will show a pop-up for installing the Python and Jupyter extension. Choose Install/Enable suggested extensions. After the extensions are installed, VS Code will automatically launch the kernel selector. You can also choose Select Kernel on the right to view the list of kernels. For the next steps, follow the directions for the space you‚Äôre connected to. Code Editor spaces: Select Python environments‚Ä¶ and choose from a list of provided Python environments. After you are connected, you can start running the cells in your notebook.\nJupyterLab spaces: Select the Existing Jupyter Server‚Ä¶ option to have the same kernel experience as the JupyterLab environment. If this is the first time connecting to JupyterLab spaces, you will need to configure the Jupyter server to view the same kernels as the remote server using the following steps.\nChoose Enter the URL of the running Jupyter Server and enter http://localhost:8888/jupyterlab/default/lab as the URL and press Enter. Enter a custom server display name, for example, JupyterLab Space Default Server and press Enter.You will now be able to view the list of kernels that‚Äôs available on the remote Jupyter server. For consequent connections, this display name will be available for you to choose from when you select the existing Jupyter server option. The following graphic shows the entire workflow. In this example, we‚Äôre running a JupyterLab space with the SageMaker Distribution image, so we can view the list of kernels available in the image.\nYou can choose the kernel of your choice, for example, the Python 3 kernel, and you can start running the notebook cells on the remote kernel. With access to the SageMaker managed kernels, you can now focus on model development rather than infrastructure and runtime management, while using the development environment you know and trust.\nBest practices and guardrails Follow the principle of least privilege when allowing users to connect remotely to SageMaker Studio spaces applications. SageMaker Studio supports custom tag propagation, we recommend tagging each user with a unique identifier and using the tag to allow the StartSession API to only their private applications. As an administrator, if you want to disable this feature for your users, you can enforce it using the sagemaker:RemoteAccess condition key. The following is an example policy. { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCreateSpaceWithRemoteAccessDisabled\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;sagemaker:CreateSpace\u0026#34;, \u0026#34;sagemaker:UpdateSpace\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;StringEquals\u0026#34;: { \u0026#34;sagemaker:RemoteAccess\u0026#34;: [\u0026#34;DISABLED\u0026#34;] } } }, { \u0026#34;Sid\u0026#34;: \u0026#34;AllowCreateSpaceWithNoRemoteAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [\u0026#34;sagemaker:CreateSpace\u0026#34;, \u0026#34;sagemaker:UpdateSpace\u0026#34;], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Condition\u0026#34;: { \u0026#34;Null\u0026#34;: { \u0026#34;sagemaker:RemoteAccess\u0026#34;: \u0026#34;true\u0026#34; } } } ] } When connecting remotely to the SageMaker Studio spaces from your local IDE, be aware of bandwidth constraints. For optimal performance, avoid using the remote connection to transfer or access large datasets. Instead, use data transfer methods built for cloud and in-place data processing to facilitate a smooth user experience. We recommend an instance with at least 8 GB of storage to start with, and the SageMaker Studio UI will throw an exception if you choose a smaller instance. Cleanup If you have created a SageMaker Studio domain for the purposes of this post, remember to delete the applications, spaces, user profiles, and the domain. For instructions, see Delete a domain. For the SageMaker Studio spaces, use the idle shutdown functionality to avoid incurring charges for compute when it is not in use.\nConclusion The remote IDE connection feature for Amazon SageMaker Studio bridges the gap between local development environments and powerful ML infrastructure of SageMaker AI. With direct connections from local IDEs to SageMaker Studio spaces, developers and data scientists can now:\nMaintain their preferred development environment while using the compute resources of SageMaker AI Use custom extensions, debugging tools, and familiar workflows Access governed data and ML resources within existing security boundaries Choose between convenient deep linking or AWS Toolkit connection methods Operate within enterprise-grade security controls and permissions This integration minimizes the productivity barriers of context switching while facilitating secure access to SageMaker AI resources. Get started today with SageMaker Studio remote IDE connection to connect your local development environment to SageMaker Studio and experience streamlined ML development workflows using your familiar tools while the powerful ML infrastructure of SageMaker AI.\nAbout the authors Durga Sury is a Senior Solutions Architect at Amazon SageMaker, where she helps enterprise customers build secure and scalable AI/ML systems. When she‚Äôs not architecting solutions, you can find her enjoying sunny walks with her dog, immersing herself in murder mystery books, or catching up on her favorite Netflix shows.\nEdward Sun is a Senior SDE working for SageMaker Studio at Amazon Web Services. He is focused on building interactive ML solution and simplifying the customer experience to integrate SageMaker Studio with popular technologies in data engineering and ML landscape. In his spare time, Edward is big fan of camping, hiking, and fishing, and enjoys spending time with his family.\nRaj Bagwe is a Senior Solutions Architect at Amazon Web Services, based in San Francisco, California. With over 6 years at AWS, he helps customers navigate complex technological challenges and specializes in Cloud Architecture, Security and Migrations. In his spare time, he coaches a robotics team and plays volleyball. He can be reached at X handle @rajesh_bagwe.\nSri Aakash Mandavilli is a Software Engineer on the Amazon SageMaker Studio team, where he has been building innovative products since 2021. He specializes in developing various solutions across the Studio service to enhance the machine learning development experience. Outside of work, SriAakash enjoys staying active through hiking, biking, and taking long walks.\n"},{"uri":"https://nhutruong47.github.io/aws/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Objectives for Week 3: Clearly understand the role and core components of the Amazon Virtual Private Cloud (VPC) virtual network service. [Image of AWS VPC architecture diagram]\nMaster the creation and configuration of VPC, Subnets (Public/Private), Internet Gateway (IGW), and Route Tables. Master the two-layer security mechanism: Security Group (SG) and Network Access Control List (NACL). Practice deploying an EC2 Instance into a custom VPC. Tasks to be implemented this week: Day Task Start Date Completion Date Resources 2 - Read and understand VPC architecture: Concepts of IP, CIDR Block, Subnet, Availability Zone (AZ). - Practice: Create a new VPC with a custom CIDR range (e.g., 10.0.0.0/16). - Create two Subnets: one Public and one Private. 22/09/2025 22/09/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn about Internet Gateway (IGW), Route Table, and their roles in providing Internet access. - Practice: + Create and attach IGW to the VPC. + Configure Public Route Table to route traffic to IGW. + Associate the Public Route Table with the Public Subnet. 23/09/2025 23/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn about the Security Group (SG) security mechanism (Stateful) and its operating principles. - Practice: + Launch an EC2 Instance in the Public Subnet. + Configure SG to only allow SSH/RDP (Port 22/3389) from personal IP. + Attempt access to verify SG security features. 24/09/2025 24/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn about the Network Access Control List (NACL) security mechanism (Stateless). - Practice: + Configure NACL for the Public Subnet (test creating Deny rules). + Detailed differentiation between SG and NACL when handling inbound/outbound traffic. + Learn the concept of NAT Gateway (in Private Subnet). 25/09/2025 25/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Cleanup \u0026amp; General Review: - Practice: + Terminate the EC2 Instance. + Delete all VPC components created in the correct order (Detach IGW -\u0026gt; Delete Subnets -\u0026gt; Delete Route Tables -\u0026gt; Delete VPC) to clean up costs. + Summarize the VPC components learned. 26/09/2025 26/09/2025 https://cloudjourney.awsstudygroup.com/ Results achieved in Week 3: VPC Configuration: Understood and manually created VPC, Subnets (Public/Private), Internet Gateway, and Route Tables to route traffic. Network Security: Mastered and distinguished the two-layer security mechanism: Security Group (Stateful, operates at the Instance level) and NACL (Stateless, operates at the Subnet level). Basic Deployment: Successfully deployed an EC2 Instance in a self-created VPC and verified Internet connectivity. Cleanup Skills: Successfully performed network resource cleanup in the correct order (ensuring no unnecessary resources are left behind), contributing to effective cost management. "},{"uri":"https://nhutruong47.github.io/aws/5-workshop/5.4-s3-onprem/","title":"Access S3 from on-premises","tags":[],"description":"","content":"Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "},{"uri":"https://nhutruong47.github.io/aws/4-eventparticipated/4.4-event4/","title":"Event 4: AWS Cloud Mastery Series #2 - DevOps on AWS","tags":[],"description":"","content":"Summary Report: ‚ÄúAWS Cloud Mastery Series #2: DevOps on AWS‚Äù Date and Topic Date and Time: Monday, November 17, 2025 (1:00 PM ‚Äì 4:30 PM) Topic: DevOps on AWS Event Objectives To provide deep understanding of the principles, processes, and best practices of DevOps on the AWS platform. To introduce and guide the use of AWS Developer Tools (CodeCommit, CodeBuild, CodeDeploy, CodePipeline) to build an automated CI/CD pipeline. To focus on advanced deployment strategies such as Blue/Green Deployment and Canary Release to mitigate risk. To explore how to integrate Security (DevSecOps) and Monitoring into the DevOps lifecycle. Speakers Detailed speaker information is not available in the original data, assumed to be AWS Vietnam Solution Architects/Specialists. Highlights 1. DevOps Principles and AWS Developer Tools DevOps Culture \u0026amp; Principles: Focus on automation, monitoring, and continuous improvement. Building CI/CD: Guidance on using AWS CodePipeline to create a continuous deployment flow from code commit to production. Infrastructure as Code (IaC): Introduction to AWS CloudFormation and AWS CDK for managing infrastructure resources as code. 2. Advanced Deployment Strategies Zero-Downtime Deployment: Analysis of advanced deployment techniques: _ Blue/Green Deployment: Deploying the new version parallel to the old version, then switching traffic. _ Canary Release: Deploying the new version to a small subset of users before wider rollout. AWS Services in Action: How to use AWS services such as Amazon ECS, Amazon EKS, and AWS Lambda to support these strategies. 3. Monitoring, Logging, and DevSecOps Observability: Using Amazon CloudWatch and AWS X-Ray to collect metrics, logs, and traces, helping to quickly detect and resolve issues. DevSecOps: Integrating security tools (e.g., Amazon Inspector, Amazon GuardDuty) into the Build and Deploy phases to detect vulnerabilities early (\u0026ldquo;Shift Left\u0026rdquo;). Key Takeaways Core DevOps Skills Professional CI/CD Design: Mastered how to design a robust and resilient CI/CD Pipeline using the AWS toolset. Minimizing Deployment Risk: Understood the pros and cons of Blue/Green and Canary Release, and when to apply each strategy to ensure Zero Downtime. Security and Observability DevSecOps Mindset: Grasped the importance of \u0026ldquo;Shift Left\u0026rdquo; ‚Äì integrating security as early as possible in the development lifecycle. Observability: Understood the difference between traditional Monitoring and modern Observability, and how to use Logs, Metrics, Traces for a comprehensive view of system performance. Application to Work Deployment Automation: Implementing CodePipeline for new projects to automate the entire Build, Test, and Deploy process. Improving Go-Live Strategy: Proposing and applying Canary Release strategies for critical features to ensure stability and gather early user feedback. Enhancing Observability: Integrating X-Ray for end-to-end request tracing, helping to debug microservices faster. Event Experience This event was a practical session, emphasizing the shift from theory to implementation within the AWS environment.\nPractical Value: The deep dive into advanced deployment strategies provided direct knowledge applicable to production environments. Tool Ecosystem: Gained a clearer understanding of how tools in the AWS ecosystem (Code* services, CloudWatch, X-Ray) work together to form a complete DevOps process. This event strengthened my knowledge of building, deploying, and operating software efficiently, securely, and automatically on AWS.\n"},{"uri":"https://nhutruong47.github.io/aws/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":" In this section, you need to list and describe in detail the events you participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3‚Ä¶, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event‚Äôs content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in 5 events (including 2 separate sessions of Vietnam Cloud Day), with each event being a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Vietnam Cloud Day 2025 - Keynote Address (Leadership Strategy)\nDate and Time: Thursday, September 18, 2025 (9:00 AM ‚Äì 12:00 PM)\nLocation: Amazon Web Services Vietnam, 36th Floor, 2 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\nBrief Content Description: Keynote speeches from senior AWS leaders and CEOs (Techcombank, U2U Network). Content focused on Cloud strategy guidance and an executive panel discussion on Navigating the GenAI Revolution.\nOutcomes or Value Gained: Elevated strategic vision, understood the complexity of integrating AI into core business decisions, and gained perspective from leaders in a rapidly changing technological environment.\nEvent 2 Event Name: Vietnam Cloud Day 2025 - Track for Builders (GenAI and Data Technology)\nDate and Time: Thursday, September 18, 2025 (1:00 PM ‚Äì 5:00 PM)\nLocation: Amazon Web Services Vietnam, 36th Floor, 2 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\nBrief Content Description: Deep-dive technical sessions on building a Unified Data Foundation, the GenAI Adoption roadmap, the AI-Driven Development Lifecycle (AI-DLC) model, and principles for Securing GenAI Applications (Zero-Trust, Encryption).\nOutcomes or Value Gained: Grasped modern Data Foundation and GenAI architectures. Understood how to protect GenAI across all layers (Model, Infrastructure) and the potential of AI-DLC in accelerating software development.\nEvent 3 Event Name: AWS Cloud Mastery Series #1: AI/ML/GenAI on AWS\nDate and Time: Saturday, November 15, 2025 (8:30 AM ‚Äì 12:00 PM)\nLocation: AWS Vietnam Office, Bitexco Financial Tower, 2 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\nBrief Content Description: Focused on Amazon SageMaker (end-to-end ML platform) and Generative AI via Amazon Bedrock. Key topics included Prompt Engineering techniques, the Retrieval-Augmented Generation (RAG) architecture, and using Foundation Models.\nOutcomes or Value Gained: Mastered the steps for end-to-end ML deployment and gained proficiency with Bedrock. Understood the RAG architecture as a key factor for applying GenAI to real-world private data applications.\nEvent 4 Event Name: AWS Cloud Mastery Series #2: DevOps on AWS\nDate and Time: Monday, November 17, 2025 (8:30 AM ‚Äì 5:00 PM)\nLocation: AWS Vietnam Office, Bitexco Financial Tower, 2 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\nBrief Content Description: A comprehensive event on DevOps on AWS, covering building CI/CD Pipelines (AWS Code Services), utilizing Infrastructure as Code (IaC) (CloudFormation, CDK), advanced deployment strategies (Blue/Green, Canary), and Monitoring \u0026amp; Observability (CloudWatch, X-Ray).\nOutcomes or Value Gained: Designed professional CI/CD Pipelines and learned how to apply advanced deployment strategies to ensure Zero Downtime. Strengthened knowledge of DevSecOps and Observability in distributed environments.\nEvent 5 Event Name: AWS Cloud Mastery Series #3: According to AWS Well-Architected Security Pillar\nDate and Time: Saturday, November 29, 2025 (8:30 AM ‚Äì 12:00 PM)\nLocation: AWS Vietnam Office, Bitexco Financial Tower, 2 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\nBrief Content Description: Focused on the 5 core security domains of the AWS Well-Architected Framework: IAM, Detection, Infrastructure Protection, Data Protection, and Incident Response. Content covered Zero Trust principles, using GuardDuty/Security Hub, KMS, and automating incident response (IR Playbook).\nOutcomes or Value Gained: Mastered the comprehensive security structure on the Cloud. Gained knowledge on how to set up continuous threat detection and manage access rights (IAM) according to the highest AWS standards.\n"},{"uri":"https://nhutruong47.github.io/aws/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/","title":"On-premises DNS Simulation","tags":[],"description":"","content":"AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"},{"uri":"https://nhutruong47.github.io/aws/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Objectives for Week 4: Clearly understand the role, architecture, and durability of the Amazon S3 service. Master creating, managing Buckets, and applying security policies such as Bucket Policy. Master S3 storage classes (Standard, IA, Glacier) and how to use Lifecycle Rules to optimize costs. Practice deploying a static website (Static Website Hosting) on S3. Tasks to be implemented this week: Day Task Start Date Completion Date Resources 2 - Read and understand the basic architecture of Amazon S3 (Object, Key, Bucket, Region). - Learn security features: Block Public Access, Access Control List (ACL). - Practice: Create a new S3 Bucket and customize Block Public Access settings. 29/09/2025 29/09/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn about S3 Storage Classes (Standard, Standard-IA, One Zone-IA, Glacier, Deep Archive). - Learn about Lifecycle Rules to automatically transition between storage classes. - Practice: Configure Lifecycle Rule for a Bucket, transition old objects to Standard-IA. 30/09/2025 30/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn about Versioning feature and how to restore objects. - Learn about Bucket Policy to manage centralized access. - Practice: Enable Versioning for a Bucket and try deleting/restoring an object. 01/10/2025 01/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn about Static Website Hosting feature on S3. - Practice: + Upload HTML/CSS/JS files (Index.html \u0026amp; Error.html). + Enable Static Website Hosting and set up Bucket Policy to allow public access. + Access the website via the S3 Endpoint. 02/10/2025 02/10/2025 https://cloudjourney.awsstudygroup.com/ 6 - Cleanup and S3 CLI Review: - Practice: + Use AWS CLI to upload/download/sync data with S3 (aws s3 cp, aws s3 sync). + Clean up resources: Disable Versioning, delete all objects (including old versions), then delete the Bucket. 03/10/2025 03/10/2025 https://cloudjourney.awsstudygroup.com/ Results achieved in Week 4: S3 Management: Clearly understood and manually created, configured, and managed S3 Buckets. Grasped the role of Block Public Access and ACL. Cost Optimization: Mastered S3 Storage Classes and learned how to use Lifecycle Rules to automatically transition data to less expensive storage classes. Security \u0026amp; Management: Successfully practiced enabling/disabling Versioning and applying Bucket Policy to control public and IAM User access. Real-world Application: Successfully deployed a static website on S3 and accessed it via the public Endpoint. CLI Operations: Proficient in using AWS CLI to manage data on S3 (upload, download, sync). "},{"uri":"https://nhutruong47.github.io/aws/4-eventparticipated/4.5-event5/","title":"Event 5: AWS Cloud Mastery Series #3 - According to AWS Well-Architected Security Pillar","tags":[],"description":"","content":"Summary Report: ‚ÄúAWS Cloud Mastery Series #3: According to AWS Well-Architected Security Pillar‚Äù Date and Topic Date and Time: Saturday, November 29, 2025 (8:30 AM ‚Äì 12:00 PM) Topic: According to AWS Well-Architected Security Pillar Event Objectives To focus on the Security Pillar of the AWS Well-Architected Framework. To provide deep knowledge about 5 key security domains: Identity \u0026amp; Access Management (IAM), Detection, Infrastructure Protection, Data Protection, and Incident Response. To introduce best practices and services for protecting workloads on the Cloud. Speakers Detailed speaker information is not available in the original data, assumed to be AWS Vietnam Solution Architects/Specialists. Highlights 1. Security Foundation \u0026amp; IAM (Identity \u0026amp; Access Management) Core Principles: Least Privilege, Zero Trust, Defense in Depth, and the Shared Responsibility Model. Modern IAM: Managing Users, Roles, and Policies, avoiding long-term credentials, and utilizing IAM Identity Center (SSO), MFA, and Access Analyzer. 2. Detection \u0026amp; Infrastructure Protection Detection \u0026amp; Monitoring: Using CloudTrail (org-level), GuardDuty (threat detection), Security Hub, and logging (VPC Flow Logs). Network Security: VPC segmentation, distinguishing Security Groups vs NACLs, and utilizing WAF + Shield and Network Firewall. 3. Data Protection \u0026amp; Incident Response Data Protection: Key management and encryption using KMS (Key Management Service) for data at-rest and in-transit (S3, RDS). Utilizing Secrets Manager to store and rotate secrets. Incident Response (IR): The IR lifecycle process and Playbooks for responding to scenarios like compromised IAM keys or S3 public exposure, utilizing Lambda/Step Functions for automation. Key Takeaways Comprehensive Security Knowledge 5 Security Pillars: Gained a complete understanding of the comprehensive security structure on the Cloud, from identity protection to incident response. Modern IAM: Understood how to design IAM architecture that avoids long-term credentials to mitigate risk. Data Protection: Mastered the essential standards for encryption and key management for sensitive data. IR Automation: Understood the role of automation in rapidly responding to security incidents (Auto-response). Application to Work Security Audit: Applying the principles of the Security Pillar to audit and improve the current security configurations of ongoing projects. GuardDuty \u0026amp; Security Hub Setup: Deploying these AWS services in the account to establish a continuous threat detection mechanism. Secrets Management: Using AWS Secrets Manager and Parameter Store to store sensitive information instead of hardcoding, and establishing rotation mechanisms. Event Experience Importance of Security: The event emphasized that security is a shared responsibility and must be integrated from the design phase (security by design). Learning Through Practice: The Mini Demos on validating IAM Policies and incident response were highly practical, demonstrating how AWS tools function. Mindset Shift: Helped transition from a \u0026ldquo;blocking\u0026rdquo; mindset to a \u0026ldquo;detection and automated response\u0026rdquo; mindset. This event is mandatory for me to build any application on AWS securely and adhere to the highest standards.\n"},{"uri":"https://nhutruong47.github.io/aws/5-workshop/5.5-policy/","title":"VPC Endpoint Policies","tags":[],"description":"","content":"When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. { \u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;, \u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34; ], \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; } ] } Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"},{"uri":"https://nhutruong47.github.io/aws/5-workshop/","title":"Workshop","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "},{"uri":"https://nhutruong47.github.io/aws/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Objectives for Week 5: Clearly understand the role and benefits of Amazon RDS service compared to self-managed databases on EC2. Master creating and connecting to a DB Instance (e.g., MySQL or PostgreSQL). Master key features: Multi-AZ (High Availability), Read Replicas (Read Scalability), and Snapshot/Backup. Practice managing network security for DB Instances via Security Groups. Tasks to be implemented this week: Day Task Start Date Completion Date Resources 2 - Read and understand the architecture of Amazon RDS (DB Instance, Engine, Master Username/Password, DB Security Group). - Compare RDS with databases on EC2. - Practice: Launch a DB Instance (select Free Tier) in a VPC created in Week 3 (select Private Subnet). 06/10/2025 06/10/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn about connection and network security for RDS. - Practice: + Configure Security Group for RDS to only allow access from EC2 Instance Security Group (created in Week 2). + Use a Client tool or EC2 to connect and create a basic table. 07/10/2025 07/10/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn about Multi-AZ Deployment feature to ensure High Availability (HA). [Image of AWS RDS Multi-AZ architecture] - Learn about Automated Backups and DB Snapshots (Manual backup). - Practice: Enable Multi-AZ for the Instance and create a manual DB Snapshot. | 08/10/2025 | 08/10/2025 | https://cloudjourney.awsstudygroup.com/ | | 5 | - Learn about Read Replicas to offload the DB Master. - Practice: + Create a Read Replica for the primary DB Instance. + Simulate application connection to Read Replica to scale read capacity. + Learn about promoting Read Replica to Standalone DB. | 09/10/2025 | 09/10/2025 | https://cloudjourney.awsstudygroup.com/ | | 6 | - Cleanup and Optimization: - Practice: + Delete Read Replica. + Delete primary DB Instance (Note: Uncheck create Final Snapshot if not needed). + Clean up all related Snapshots and Security Groups. | 10/10/2025 | 10/10/2025 | https://cloudjourney.awsstudygroup.com/ |\nResults achieved in Week 5: RDS Management: Clearly understood the benefits of RDS and proficient in creating, configuring, and connecting to DB Instances. Connection Security: Mastered using Security Groups to only allow specific resources (like EC2) to access the database, ensuring network layer security. High Availability (HA): Understood and successfully practiced deploying Multi-AZ to protect the database from Availability Zone (AZ) failures. Scalability: Understood and practiced creating Read Replicas to optimize read performance and offload the DB Master. Backup \u0026amp; Recovery: Grasped the process of creating DB Snapshots and how RDS performs Automated Backups. "},{"uri":"https://nhutruong47.github.io/aws/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"Throughout my internship at Amazon Web Services (AWS) Vietnam (under the First Cloud Journey program) from September 8, 2025 until now, I have had the opportunity to learn, train, and apply the knowledge acquired at school to a real-world work environment.\nI have participated in researching and deploying Cloud Computing solutions, completing specialized Labs, and passing the Midterm exam, thereby improving my skills in Full-stack Programming (Java/React), System Deployment, Deep Dive thinking, and Time Management.\nRegarding professional conduct, I have consistently strived to complete tasks well and actively communicated with my Mentor to enhance work efficiency. To objectively reflect upon my internship process, I assess myself based on the criteria below:\nNo. Criteria Description Excellent Good Average 1 Knowledge and Professional Skills Understanding of the industry, application of knowledge to practice, tool proficiency, work quality ‚úÖ ‚òê ‚òê 2 Learning Agility Assimilating new knowledge, learning quickly ‚òê ‚úÖ ‚òê 3 Proactiveness Self-researching, taking tasks without waiting for instruction ‚úÖ ‚òê ‚òê 4 Sense of Responsibility Completing work on time, ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Adhering to schedule, rules, and work procedures ‚òê ‚òê ‚úÖ 6 Growth Mindset Being receptive to feedback and self-improvement ‚òê ‚úÖ ‚òê 7 Communication Presenting ideas, reporting work clearly ‚òê ‚úÖ ‚òê 8 Teamwork Working effectively with colleagues, participating in the team ‚úÖ ‚òê ‚òê 9 Professional Conduct Respecting colleagues, partners, and the work environment ‚úÖ ‚òê ‚òê 10 Problem-Solving Skills Identifying issues, proposing solutions, creativity ‚òê ‚úÖ ‚òê 11 Contribution to Project/Organization Work effectiveness, innovative suggestions, recognition from the team ‚úÖ ‚òê ‚òê 12 Overall General assessment of the entire internship process ‚úÖ ‚òê ‚òê Personal Improvement Plan Based on the assessment table above, I recognize several areas that need improvement to become a more professional engineer:\nEnhance Discipline and Process Compliance:\nI realize I need to be more serious about adhering to organizational hours and administrative regulations. - Action: Establish a tighter personal work schedule (Timeboxing), ensure punctuality for all meetings (Sync-ups), and strictly adhere to AWS security/procedural guidelines.\nImprove Problem-Solving Skills:\nCurrently, I tend to focus on finding a quick fix rather than investigating the root cause. - Action: Rigorously apply the \u0026ldquo;Deep Dive\u0026rdquo; mindset. When encountering an error, instead of just making the code run, I will dedicate time to analyze why the error occurred and how to prevent it in the future (using the 5 Whys methodology).\nBoost Communication Skills: - I sometimes struggle when presenting complex technical ideas or when asking for support. - Action: Practice formulating concise questions with clear context when seeking assistance from the Mentor. Concurrently, proactively participate more in technical sharing sessions to build confidence.\n"},{"uri":"https://nhutruong47.github.io/aws/5-workshop/5.6-cleanup/","title":"Clean up","tags":[],"description":"","content":"Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "},{"uri":"https://nhutruong47.github.io/aws/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Objectives for Week 6: Clearly understand the architecture and advantages of the Amazon DynamoDB NoSQL database (Key-Value/Document). Master creating, managing DynamoDB tables, and using Index types (GSI/LSI). Master the role of Amazon ElastiCache in accelerating application response speeds. Practice creating and configuring ElastiCache Clusters (Redis/Memcached). Tasks to be implemented this week: Day Task Start Date Completion Date Resources 2 - Read and understand Amazon DynamoDB architecture (Primary Key, Partition Key, Sort Key). [Image of Amazon DynamoDB architecture] - Learn concepts of Read/Write Capacity Units (RCU/WCU) and On-Demand mode. - Practice: Create the first DynamoDB table, insert and query basic data. | 13/10/2025 | 13/10/2025 | https://cloudjourney.awsstudygroup.com/ | | 3 | - Deep dive into Index types: Local Secondary Index (LSI) and Global Secondary Index (GSI). - Practice: Create GSI for a DynamoDB table to support non-primary key queries. | 14/10/2025 | 14/10/2025 | https://cloudjourney.awsstudygroup.com/ | | 4 | - Learn about Amazon ElastiCache in-memory caching service (Redis and Memcached). - Analyze the benefits of using a caching layer to offload RDS/DynamoDB. - Practice: Launch an ElastiCache Cluster (e.g., Redis) in a Private Subnet. | 15/10/2025 | 15/10/2025 | https://cloudjourney.awsstudygroup.com/ | | 5 | - Learn how to configure Security Group for ElastiCache to ensure secure connections from EC2. - Learn how applications connect and perform basic data operations on ElastiCache. - Practice: Configure Security Group for the ElastiCache Cluster. | 16/10/2025 | 16/10/2025 | https://cloudjourney.awsstudygroup.com/ | | 6 | - Resource Cleanup \u0026amp; Review: - Practice: + Delete the created DynamoDB table. + Delete the ElastiCache Cluster. + Summarize pros/cons and Use-cases of RDS, DynamoDB, and ElastiCache. | 17/10/2025 | 17/10/2025 | https://cloudjourney.awsstudygroup.com/ |\nResults achieved in Week 6: DynamoDB Fundamentals: Clearly understood the NoSQL model of DynamoDB, proficient in creating, managing tables and primary key types. Query \u0026amp; Cost Optimization: Grasped the role of GSI/LSI and capacity modes (On-Demand/Provisioned) in DynamoDB. ElastiCache Fundamentals: Understood the differences and benefits of Redis/Memcached. Grasped the role of caching in application architecture. [Image of AWS ElastiCache architecture]\nCaching Deployment: Successfully launched an ElastiCache Cluster and set up network security via Security Group. Service Differentiation: Capable of comparing and selecting between RDS, DynamoDB, and ElastiCache for different data requirements. "},{"uri":"https://nhutruong47.github.io/aws/7-feedback/","title":"Feedback and Suggestions","tags":[],"description":"","content":"Overall Assessment 1. Work Environment The environment at FCJ is highly dynamic and fosters a proactive mindset (Ownership). I was particularly impressed with the culture of \u0026ldquo;Customer Obsession\u0026rdquo; and the way people dive deep into problems (Deep Dive). The workspace is conducive to concentration, but the greatest value lies in the open academic atmosphere, where every idea regarding new technologies (like GenAI or Cloud Native) is encouraged and debated enthusiastically.\n2. Support from Mentor / Team Admin Mentors not only serve as guides but also as sources of inspiration. I highly value the support received from the mentors during the preparation phase for the recent Midterm exam (October 31st). The mentors were patient in explaining difficult concepts regarding system architecture and deployment, helping me not just pass the exam but truly understand the core essence of the problem rather than merely memorizing facts.\n3. Alignment between Job and Academic Major The program is the perfect complement to the knowledge I have accumulated (Java Spring Boot, ReactJS, Python). FCJ helped me upgrade my thinking from just writing code that runs on a local machine (localhost) to a mindset of deploying systems on the Cloud, gaining clarity on VPS, DNS, and how a real-world application operates in the Internet environment. This is a crucial transformation for an engineering student.\n4. Opportunity for Learning \u0026amp; Skill Development This is the biggest highlight. Besides foundational knowledge, I had the opportunity to access the most cutting-edge technologies through events like the AWS Cloud Mastery Series. Being able to delve deep into AI/ML, DevOps, and Security (during the November workshops) significantly broadened my perspective, helping me clearly define my path toward becoming a Cloud and AI-savvy Full-stack Developer.\n5. Culture \u0026amp; Team Spirit The spirit of \u0026ldquo;Work hard, Have fun, Make history\u0026rdquo; is clearly evident. Although the pressure from Labs and exams was considerable, the support among the interns was excellent. Everyone frequently shared documentation, collaborated to debug deployment errors, and motivated each other during peak weeks.\n6. Intern Policies / Benefits I am satisfied with the program\u0026rsquo;s support, especially the opportunity to attend specialized AWS workshops and conferences (like AWS Innovate). These are non-material benefits but provide immense knowledge value that is hard to find in typical internship programs.\nOther Questions What did you enjoy most during the internship? It was the opportunity to participate in the AWS Cloud Mastery Series and the Generative AI training sessions. It ensured I wasn\u0026rsquo;t limited to just basic programming tasks but was exposed to the global technological landscape.\nWhat do you think the company should improve for subsequent interns? I propose the program could increase the number of practical, hands-on labs focusing more on Deployment and CI/CD right from the initial phase, to help subsequent interns be less overwhelmed when pushing products to the production environment.\nIf recommending to friends, would you advise them to intern here? Why? Definitely YES. This is not just a workplace but a \u0026ldquo;second university\u0026rdquo; teaching the most practical knowledge about Cloud Computing that the curriculum at school has yet to update.\nSuggestions \u0026amp; Aspirations Do you have any suggestions for improving the internship experience? I hope there will be more internal mini-hackathons to apply the learned DevOps and AI knowledge to a complete product within a short timeframe (24h-48h). Would you like to continue this program in the future? I am very keen to continue contributing and advancing further within the AWS ecosystem. Other feedback (free sharing): Thank you to the FCJ team for creating such a memorable journey from the early days of September until now. "},{"uri":"https://nhutruong47.github.io/aws/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Objectives for Week 7: Clearly understand the role and configuration of EC2 Auto Scaling Group (ASG) to automatically scale resources out and in. [Image of AWS Auto Scaling architecture]\nMaster the architecture and operation of Elastic Load Balancer (ELB) (especially ALB) to distribute traffic. Proficient in using Amazon CloudWatch to collect metrics, logs, create Dashboards, and set up Alarms. Practice setting up Scaling Policies based on monitoring metrics. Tasks to be implemented this week: Day Task Start Date Completion Date Resources 2 - Read and understand Auto Scaling Group (ASG) architecture, Launch Template. - Learn about Load Balancer types (Classic, Application, Network). - Practice: Create a Launch Template and configure a basic ASG. 20/10/2025 20/10/2025 https://cloudjourney.awsstudygroup.com/ 3 - Deep dive into Application Load Balancer (ALB) and components (Listener, Target Group, Health Check). [Image of AWS Application Load Balancer components] - Practice: + Create ALB, Target Group. + Register ASG EC2 Instances into the Target Group. | 21/10/2025 | 21/10/2025 | https://cloudjourney.awsstudygroup.com/ | | 4 | - Learn about Amazon CloudWatch monitoring service (Metrics, Logs, Events). - Practice: + View EC2, ALB, ASG Metrics on CloudWatch. + Create a CloudWatch Dashboard to track key metrics (CPU Utilization, Request Count). | 22/10/2025 | 22/10/2025 | https://cloudjourney.awsstudygroup.com/ | | 5 | - Learn about CloudWatch Alarms and Scaling Policy types (Target Tracking, Step Scaling). - Practice: + Create CloudWatch Alarm based on CPU Utilization (e.g., \u0026gt; 80%). + Configure Scaling Policy for ASG using the created Alarm (scale out when CPU is high). | 23/10/2025 | 23/10/2025 | https://cloudjourney.awsstudygroup.com/ | | 6 | - Check \u0026amp; Resource Cleanup: - Practice: + Verify Scale Out/In process is working. + Cleanup: Delete CloudWatch Alarms, delete ASG (will automatically terminate EC2), delete ALB, delete Launch Template. | 24/10/2025 | 24/10/2025 | https://cloudjourney.awsstudygroup.com/ |\nResults achieved in Week 7: Scalability: Successfully deployed application architecture capable of automatic scaling (Scale Out/In) via ASG and Launch Template. Load Balancing: Understood and configured Application Load Balancer (ALB) to distribute traffic and perform effective Health Checks. Monitoring (Observability): Mastered using CloudWatch to collect, visualize (Dashboard), and track resource health metrics. Automation: Successfully established performance-based auto-scaling mechanisms (Scaling Policy and CloudWatch Alarms). Cost Management: Mastered the process of cleaning up expensive resources like ALB and ASG. "},{"uri":"https://nhutruong47.github.io/aws/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Objectives for Week 8: Clearly understand the architecture, advantages, and core components of the serverless compute service AWS Lambda. [Image of AWS Lambda architecture components]\nMaster creating, deploying, and managing Lambda functions. Master the role and configuration of Amazon API Gateway to create RESTful APIs. Practice building a simple Serverless application by integrating Lambda and API Gateway. Tasks to be implemented this week: Day Task Start Date Completion Date Resources 2 - Read and understand AWS Lambda architecture (Function, Runtime, Handler, Execution Role). - Compare Serverless (Lambda) with Compute (EC2). - Practice: Create and deploy a simple Lambda function (e.g., print \u0026ldquo;Hello World\u0026rdquo;). 27/10/2025 27/10/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn about the Event-Driven mechanism in Lambda. - Practice: + Configure a Trigger for Lambda to be activated by an S3 event (e.g., file upload). + Configure Lambda to read/write data from DynamoDB (using IAM Role). 28/10/2025 28/10/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn about Amazon API Gateway service (REST API, Stage, Resource, Method). - Practice: Create a new REST API on API Gateway. - Define basic Resources and Methods (e.g., GET /items, POST /items). 29/10/2025 29/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn about Integration Type (Lambda Proxy Integration) between API Gateway and Lambda. - Practice: + Connect Method POST /items of API Gateway with the Lambda function created on day 2. + Deploy the API to a Stage (e.g., prod) and test via the public URL. 30/10/2025 30/10/2025 https://cloudjourney.awsstudygroup.com/ 6 - Security, Monitoring \u0026amp; Cleanup: - Learn about CORS (Cross-Origin Resource Sharing) and how to enable it on API Gateway. - Practice: + Enable CORS for the API. + Check Lambda Logs on CloudWatch. + Delete API Gateway, then delete the Lambda function. 31/10/2025 31/10/2025 https://cloudjourney.awsstudygroup.com/ Results achieved in Week 8: Lambda Fundamentals: Clearly understood the Serverless model, proficient in creating, configuring, and managing Lambda functions (Function, Role, Runtime). Event-Driven: Understood how Lambda operates based on events and practiced integrating with other services like S3 and DynamoDB. API Gateway: Mastered API Gateway architecture (Resource, Method, Stage) and the ability to create RESTful APIs. Serverless Construction: Successfully deployed a basic Serverless architecture (API Gateway -\u0026gt; Lambda -\u0026gt; DynamoDB), a popular architecture pattern on AWS. Security \u0026amp; Cleanup: Grasped how to manage Lambda Logs via CloudWatch and performed Serverless resource cleanup. "},{"uri":"https://nhutruong47.github.io/aws/1-worklog/1.9-week9/","title":"Worklog Tu·∫ßn 9","tags":[],"description":"","content":"M·ª•c ti√™u tu·∫ßn 9: Hi·ªÉu r√µ ki·∫øn tr√∫c v√† vai tr√≤ c·ªßa d·ªãch v·ª• x·ª≠ l√Ω d·ªØ li·ªáu truy·ªÅn t·∫£i Amazon Kinesis (Data Streams, Firehose). N·∫Øm v·ªØng c√°c kh√°i ni·ªám v·ªÅ AWS Glue (Data Catalog, Crawler, ETL Jobs) trong quy tr√¨nh ETL (Extract, Transform, Load). Th√†nh th·∫°o vi·ªác t·∫°o v√† qu·∫£n l√Ω Glue Data Catalog ƒë·ªÉ l∆∞u tr·ªØ Metadata v·ªÅ d·ªØ li·ªáu. Th·ª±c h√†nh k·∫øt n·ªëi Kinesis Firehose ƒë·ªÉ truy·ªÅn t·∫£i d·ªØ li·ªáu Stream v√†o S3. C√°c c√¥ng vi·ªác c·∫ßn tri·ªÉn khai trong tu·∫ßn n√†y: Th·ª© C√¥ng vi·ªác Ng√†y b·∫Øt ƒë·∫ßu Ng√†y ho√†n th√†nh Ngu·ªìn t√†i li·ªáu 2 - ƒê·ªçc v√† hi·ªÉu ki·∫øn tr√∫c c·ªßa Amazon Kinesis Data Streams (KDS) (Shard, Producer, Consumer). - T√¨m hi·ªÉu v·ªÅ Amazon Kinesis Data Firehose ƒë·ªÉ ƒë∆°n gi·∫£n h√≥a vi·ªác n·∫°p d·ªØ li·ªáu. - Th·ª±c h√†nh: T·∫°o m·ªôt Kinesis Data Firehose Delivery Stream. 03/11/2025 03/11/2025 https://cloudjourney.awsstudygroup.com/ 3 - T√¨m hi·ªÉu v·ªÅ AWS Glue Data Catalog (Kho l∆∞u tr·ªØ Metadata) v√† Glue Crawler. - Th·ª±c h√†nh: + T·∫°o m·ªôt IAM Role cho Glue Crawler. + T·∫°o Glue Crawler ƒë·ªÉ qu√©t d·ªØ li·ªáu m·∫´u trong S3 Bucket (t·ª´ tu·∫ßn 4) v√† c·∫≠p nh·∫≠t Data Catalog. 04/11/2025 04/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - T√¨m hi·ªÉu v·ªÅ AWS Glue ETL Jobs (Extract, Transform, Load) v√† m√¥i tr∆∞·ªùng th·ª±c thi (Spark, Python Shell). - Th·ª±c h√†nh: + T·∫°o m·ªôt Glue ETL Job c∆° b·∫£n (s·ª≠ d·ª•ng Python Shell). + C·∫•u h√¨nh Job ƒë·ªÉ ƒë·ªçc d·ªØ li·ªáu t·ª´ Data Catalog v√† ghi k·∫øt qu·∫£ ra S3. 05/11/2025 05/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Th·ª±c h√†nh T√≠ch h·ª£p Kinesis: - Th·ª±c h√†nh: + ƒê·ªãnh c·∫•u h√¨nh Kinesis Firehose Delivery Stream ƒë√£ t·∫°o ƒë·ªÉ ƒë√≠ch ƒë·∫øn l√† S3. + M√¥ ph·ªèng vi·ªác n·∫°p d·ªØ li·ªáu (data ingestion) v√†o Firehose v√† ki·ªÉm tra xem d·ªØ li·ªáu c√≥ ƒë∆∞·ª£c l∆∞u tr·ªØ tr√™n S3 hay kh√¥ng. 06/11/2025 06/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - D·ªçn d·∫πp T√†i nguy√™n \u0026amp; √în t·∫≠p: - Th·ª±c h√†nh: + X√≥a Glue Job, Glue Crawler, v√† c√°c b·∫£ng trong Data Catalog. + X√≥a Kinesis Data Firehose Delivery Stream. + T·ªïng k·∫øt vai tr√≤ c·ªßa Kinesis (T·ªëc ƒë·ªô) v√† Glue (X·ª≠ l√Ω Batch/ETL). 07/11/2025 07/11/2025 https://cloudjourney.awsstudygroup.com/ K·∫øt qu·∫£ ƒë·∫°t ƒë∆∞·ª£c tu·∫ßn 9: X·ª≠ l√Ω Streaming: Hi·ªÉu ƒë∆∞·ª£c c∆° ch·∫ø truy·ªÅn t·∫£i d·ªØ li·ªáu theo th·ªùi gian th·ª±c (real-time) v·ªõi Kinesis v√† c√°ch Firehose ƒë∆°n gi·∫£n h√≥a vi·ªác n·∫°p d·ªØ li·ªáu. Qu·∫£n l√Ω Metadata: Th√†nh th·∫°o vi·ªác s·ª≠ d·ª•ng Glue Data Catalog v√† Crawler ƒë·ªÉ kh√°m ph√° (discover) v√† qu·∫£n l√Ω Metadata cho d·ªØ li·ªáu trong S3. ETL C∆° b·∫£n: N·∫Øm ƒë∆∞·ª£c quy tr√¨nh Extract, Transform, Load (ETL) v√† ƒë√£ th·ª±c h√†nh t·∫°o Glue ETL Job ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu. T√≠ch h·ª£p: Th·ª±c h√†nh th√†nh c√¥ng vi·ªác truy·ªÅn d·ªØ li·ªáu t·ª´ ngu·ªìn (Firehose) ƒë·∫øn ƒë√≠ch (S3) v√† x·ª≠ l√Ω d·ªØ li·ªáu th√¥. Kh√°i ni·ªám Big Data: Ph√¢n bi·ªát ƒë∆∞·ª£c c√°c d·ªãch v·ª• ph√π h·ª£p cho ph√¢n t√≠ch d·ªØ li·ªáu Batch (Glue) v√† Streaming (Kinesis). "},{"uri":"https://nhutruong47.github.io/aws/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Objectives for Week 10: Clearly understand the role and operation of the Amazon Simple Queue Service (SQS) message queue service (Standard \u0026amp; FIFO). Master the Publish/Subscribe architecture with the Amazon Simple Notification Service (SNS) notification service. [Image of AWS SNS architecture]\nProficient in creating and configuring AWS Step Functions state machines to orchestrate AWS services. Practice building an asynchronous workflow using SQS, SNS, and Lambda. Tasks to be implemented this week: Day Task Start Date Completion Date Resources 2 - Read and understand Amazon SQS (Message Queue) architecture and the difference between Standard Queue and FIFO Queue. - Practice: Create an SQS Standard Queue, manually send and receive messages via Console. 10/11/2025 10/11/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn about Amazon SNS notification service (Topic, Subscriber) and Pub/Sub model. - Practice: + Create an SNS Topic. + Create SQS Queue and Lambda function (from Week 8) as Subscribers for this Topic. + Send message to Topic and verify if SQS/Lambda received it. 11/11/2025 11/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn about AWS Step Functions (State Machine, Task State, Choice State). - Practice: + Create a new Lambda function (e.g., ProcessStep1). + Create a simple State Machine (with only one Task step) to invoke this Lambda function. 12/11/2025 12/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn how Step Functions orchestrate logic flow (Sequence, Choice, Parallel). - Practice: Expand the created State Machine: + Add a Choice step based on input results. + Integrate SQS (e.g., send message to queue if flow goes a certain branch). 13/11/2025 13/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Resource Cleanup \u0026amp; General Integration: - Practice: + Delete State Machine, SQS Queue, SNS Topic. + Review how these services (SQS, SNS, Step Functions) solve asynchronous communication and decoupling issues in applications. 14/11/2025 14/11/2025 https://cloudjourney.awsstudygroup.com/ Results achieved in Week 10: Message Queues: Clearly understood the role of SQS in decoupling and buffering applications. Proficient in sending/receiving messages. Pub/Sub Model: Grasped the SNS mechanism to efficiently distribute messages to multiple Subscribers. Workflow Orchestration: Clearly understood how Step Functions help orchestrate other AWS services into an organized and easily monitored workflow. Asynchronous System Construction: Capable of designing application architecture using these application integration services to increase flexibility and scalability. "},{"uri":"https://nhutruong47.github.io/aws/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Objectives for Week 11: Clearly understand the role of AWS WAF (Web Application Firewall) and AWS Shield in protecting applications from Layer 7 attacks (WAF) and DDoS (Shield). Master the operating mechanism and benefits of the intelligent threat detection service Amazon GuardDuty. Review and apply advanced security principles such as IAM Policy Best Practices and Secrets Management. Practice configuring basic security Rules. Tasks to be implemented this week: Day Task Start Date Completion Date Resources 2 - Read and understand AWS WAF architecture and Web ACL (Access Control List). [Image of AWS Web ACL architecture] - Learn about Layer 7 attacks (SQL Injection, XSS) that WAF protects against. - Practice: Create a Web ACL and learn about Managed Rule Groups. | 17/11/2025 | 17/11/2025 | https://cloudjourney.awsstudygroup.com/ | | 3 | - Learn about AWS Shield Standard/Advanced (DDoS Protection) and how it integrates with WAF/Route 53. - WAF Practice: + Create a Rate-based Rule (limit access frequency). + Test blocking specific IPs (IP Set) via WAF. | 18/11/2025 | 18/11/2025 | https://cloudjourney.awsstudygroup.com/ | | 4 | - Learn about the threat detection service Amazon GuardDuty (using Machine Learning). - Practice: Enable GuardDuty and review sample Findings (simulated threats) to understand how the service alerts. | 19/11/2025 | 19/11/2025 | https://cloudjourney.awsstudygroup.com/ | | 5 | - Review and apply IAM Policy Best Practices (Principle of Least Privilege). - Learn about AWS Secrets Manager (credential rotation) and AWS Systems Manager Parameter Store (configuration storage). - Practice: Review the IAM Policies created in Week 2. | 20/11/2025 | 20/11/2025 | https://cloudjourney.awsstudygroup.com/ | | 6 | - Summary \u0026amp; Resource Cleanup: - Practice: + Delete Web ACL/WAF Rules. + Disable GuardDuty. + Summarize security measures (Perimeter Security, Monitoring, Secrets Management) learned. | 21/11/2025 | 21/11/2025 | https://cloudjourney.awsstudygroup.com/ |\nResults achieved in Week 11: Perimeter Protection: Clearly understood how WAF and Shield work at the network edge to protect resources like CloudFront, ALB. Threat Detection: Proficient in enabling and tracking alerts from GuardDuty to detect suspicious behavior within the AWS account. Identity Management: Reinforced knowledge of IAM Policy and understood the importance of Secrets Management using specialized services. Security Construction: Grasped how to integrate security services into application architecture (e.g., using WAF in front of ALB). "},{"uri":"https://nhutruong47.github.io/aws/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Objectives for Week 12: Synthesize and consolidate all knowledge about core AWS services learned over the past 11 weeks. Assess knowledge and identify areas for improvement (Weak Areas). Plan details for the Final Project or prepare for AWS certifications. Thoroughly clean up all resources created throughout the internship. Tasks to be implemented this week: Day Task Start Date Completion Date Resources 2 - Review Core Infrastructure: Review IAM, EC2, VPC, S3. - Practice: Manually redraw a complete VPC architecture (Public/Private Subnet) and deploy EC2 with IAM Role. 24/11/2025 24/11/2025 https://cloudjourney.awsstudygroup.com/ 3 - Review Databases \u0026amp; Scaling: Review RDS, DynamoDB, ElastiCache, Auto Scaling, ELB. - Practice: Analyze a specific Use-case (e.g., E-commerce) and decide whether to use RDS, DynamoDB, or ElastiCache for each data type. 25/11/2025 25/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Review Serverless \u0026amp; Integration: Review Lambda, API Gateway, SQS, SNS, Step Functions. - Practice: Outline an asynchronous workflow for an image processing task using S3 -\u0026gt; Lambda -\u0026gt; SQS. 26/11/2025 26/11/2025 https://cloudjourney.awsstudygroup.com/ 5 - Review Security \u0026amp; Monitoring: Review CloudWatch, GuardDuty, WAF, Budget. - Practice: + Set up a Cost Budget (if not already exists). + Create CloudWatch Alarm for critical resources (CPU, Disk Usage). 27/11/2025 27/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Summary \u0026amp; Plan: - Final Resource Cleanup: Check all Regions to ensure no resources are running (Zero Resource). - Outline Final Project or build a roadmap for AWS Solutions Architect Associate certification. 28/11/2025 28/11/2025 https://cloudjourney.awsstudygroup.com/ Results achieved in Week 12: Knowledge Systematization: Completed review of all major AWS service groups, from Compute, Storage, Database, Networking to Serverless and Security. Architectural Thinking: Capable of analyzing application requirements and selecting the most appropriate AWS services to solve the problem. Cost Management (Most Important): Ensured the AWS account is completely cleaned (Zero Resource), strictly adhering to cost management principles. Career Orientation: Built a clear plan for the next steps (Project, Certification) to continue developing on the Cloud Engineer path. "},{"uri":"https://nhutruong47.github.io/aws/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://nhutruong47.github.io/aws/tags/","title":"Tags","tags":[],"description":"","content":""}]